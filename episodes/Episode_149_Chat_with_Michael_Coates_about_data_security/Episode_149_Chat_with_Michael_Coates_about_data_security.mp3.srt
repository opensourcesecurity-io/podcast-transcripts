0
00:00:05,849 --> 00:00:08,409
Hello and welcome to the open source security podcast

1
00:00:08,569 --> 00:00:12,560
episode 149 with myself Kurt Siefried and my partner in Thought Crime,

2
00:00:12,569 --> 00:00:13,319
Josh Bresser.

3
00:00:13,670 --> 00:00:14,229
-- Hey,
-- Kurt.

4
00:00:14,239 --> 00:00:17,030
And today we have a guest, we have Michael Coats,

5
00:00:17,040 --> 00:00:20,290
who is the CEO and co-founder of Altitude.

6
00:00:20,299 --> 00:00:21,190
Say hello, Michael.

7
00:00:21,319 --> 00:00:22,790
-- Hey, everyone.
-- This is Michael here.

8
00:00:23,059 --> 00:00:25,350
Awesome. So why don't you tell us a little bit about yourself, man?

9
00:00:25,559 --> 00:00:26,819
Yeah. Yeah, definitely.

10
00:00:26,829 --> 00:00:29,319
Well, man, I've, I've been in the security industry for, uh,

11
00:00:29,329 --> 00:00:30,620
it feels like a long time now.

12
00:00:30,629 --> 00:00:32,418
Probably I think 15 years.

13
00:00:32,709 --> 00:00:33,259
Nice.

14
00:00:33,470 --> 00:00:38,259
Yeah, I'm really happy to have seen security from so many different angles.

15
00:00:38,270 --> 00:00:40,770
I really got my appreciation and cut my teeth.

16
00:00:41,279 --> 00:00:45,435
Uh, you know, back in the day as, as a, as a red teamer, external,

17
00:00:45,444 --> 00:00:48,134
external and internal penetration tests for

18
00:00:48,145 --> 00:00:50,485
governments and banks every week going

19
00:00:50,494 --> 00:00:53,154
out and doing a different assessment was such a lot of fun,

20
00:00:53,165 --> 00:00:54,735
such a learning experience.

21
00:00:55,345 --> 00:00:57,955
Um But yeah, I've been in the industry a long time, um,

22
00:00:57,965 --> 00:01:01,104
had the opportunity to do some exciting things along the way.

23
00:01:01,115 --> 00:01:04,154
Um, you know, led security for Mozilla for

24
00:01:04,430 --> 00:01:05,959
about four years

25
00:01:06,139 --> 00:01:08,069
was the c soft Twitter most recently.

26
00:01:08,739 --> 00:01:10,000
And then, uh,

27
00:01:10,129 --> 00:01:13,510
yeah, just about a year ago, uh, finally made the big jump and said, hey,

28
00:01:13,519 --> 00:01:17,120
there's a big gap in the security space for a solution.

29
00:01:17,129 --> 00:01:20,849
Uh, I personally needed it real bad before. And so did my peers. So

30
00:01:21,129 --> 00:01:22,650
why not jump in and go build it?

31
00:01:22,660 --> 00:01:25,260
And that's, uh, that was the beginning of altitude networks. Yeah.

32
00:01:25,269 --> 00:01:26,559
So that's what I've been up to, traveled.

33
00:01:26,569 --> 00:01:28,900
The world worked in security, have a great family

34
00:01:29,139 --> 00:01:30,550
out here in San Francisco and

35
00:01:30,889 --> 00:01:32,110
like we all are living in the good.

36
00:01:32,730 --> 00:01:35,120
Awesome. So why don't you tell us a little bit about altitude?

37
00:01:35,129 --> 00:01:37,160
I'm I'm intrigued by what you're up to.

38
00:01:37,269 --> 00:01:38,319
Yeah, you know, I had this,

39
00:01:38,470 --> 00:01:42,500
this realization when I was at Twitter and we kept adopting

40
00:01:42,660 --> 00:01:42,669
a

41
00:01:42,959 --> 00:01:47,739
applications from Google Drive to Box and, and, and the like,

42
00:01:47,750 --> 00:01:53,440
and the problem kept coming up was how do we extend our visibility and control

43
00:01:53,910 --> 00:01:57,029
into the SAS application space?

44
00:01:57,389 --> 00:02:01,980
Um You know, every company has solid investments in their on prem security,

45
00:02:02,019 --> 00:02:03,410
uh you know, positioning.

46
00:02:03,419 --> 00:02:05,720
But when you have situations where all of your

47
00:02:05,730 --> 00:02:07,760
sensitive data is out there in the cloud,

48
00:02:07,769 --> 00:02:08,889
how do you start to control that?

49
00:02:08,899 --> 00:02:10,240
How do you know when

50
00:02:10,380 --> 00:02:13,500
people are making mistakes and sharing documents with people

51
00:02:13,509 --> 00:02:15,600
inside the company or out that they shouldn't be

52
00:02:15,860 --> 00:02:18,820
um or maybe an employee um is compromised

53
00:02:18,830 --> 00:02:22,130
and downloads thousands of documents or a contractor leaves

54
00:02:22,350 --> 00:02:25,429
but still has access via their old uh work email account.

55
00:02:25,440 --> 00:02:28,800
You can't just cut off that work email from the contractor, the other account.

56
00:02:28,809 --> 00:02:31,229
And so this started to really become a big pain and

57
00:02:31,240 --> 00:02:33,639
a big sense of a big source of risk for us.

58
00:02:33,649 --> 00:02:35,464
Looking at the solutions out there,

59
00:02:35,645 --> 00:02:39,384
there was really nothing that was integrating and providing the

60
00:02:39,395 --> 00:02:41,634
value that we were looking for at the time.

61
00:02:41,645 --> 00:02:43,425
And when I talked to my peers, um

62
00:02:43,544 --> 00:02:45,794
I'll tell you, the cio cio groups are the best.

63
00:02:45,804 --> 00:02:47,345
If nothing else, they're a support group.

64
00:02:47,964 --> 00:02:48,774
Um But we had

65
00:02:48,875 --> 00:02:52,794
fantastic co group out here in San Francisco and as I talk to them,

66
00:02:53,149 --> 00:02:55,039
uh they weren't, you know, tell me, oh,

67
00:02:55,050 --> 00:02:57,169
just go use this product or go go use this approach.

68
00:02:57,179 --> 00:02:59,039
They're saying the same things like man, we are,

69
00:02:59,250 --> 00:03:02,649
we are equally concerned about this. It's a, it's a rat's nest in there.

70
00:03:02,660 --> 00:03:04,509
We have no idea what's going on, no way to fix it.

71
00:03:04,970 --> 00:03:06,250
And so I said, well, you know,

72
00:03:06,259 --> 00:03:10,300
we could do the standard Silicon Valley thing and build another in-house tool and,

73
00:03:10,309 --> 00:03:11,250
and, and do that.

74
00:03:11,259 --> 00:03:12,250
But I thought, you know,

75
00:03:12,259 --> 00:03:14,690
there's probably a lot of other companies that need this solution.

76
00:03:15,070 --> 00:03:17,449
And so that was the beginning, jumped out there to,

77
00:03:17,770 --> 00:03:19,250
to found altitude networks.

78
00:03:19,259 --> 00:03:23,259
My, my co founder came out of capital one and a good friend of mine. Yeah.

79
00:03:23,270 --> 00:03:26,169
And together we're solving that problem of how do you integrate into

80
00:03:26,179 --> 00:03:30,179
all these SAS applications uh understand what's happening with your data,

81
00:03:30,300 --> 00:03:30,649
uh

82
00:03:30,895 --> 00:03:36,615
continuous evaluation style and then have integrated control to both fix issues

83
00:03:36,925 --> 00:03:40,054
and uh integrate policies to prevent them from happening again.

84
00:03:40,065 --> 00:03:40,274
So it's,

85
00:03:40,285 --> 00:03:42,024
it's been a wild ride so far and we're

86
00:03:42,035 --> 00:03:44,485
pretty excited about the excitement our customers are having,

87
00:03:44,494 --> 00:03:46,535
we have a product out there already, which is pretty awesome.

88
00:03:47,104 --> 00:03:50,634
Yeah, I know, I know. The uh the cloud Secure Alliance is 100% cloud based and that's,

89
00:03:51,020 --> 00:03:53,360
it's been an ongoing challenge to see

90
00:03:53,490 --> 00:03:56,520
what's going on in box.com, what's going on in Google Drive.

91
00:03:56,529 --> 00:03:58,750
-- It's very nontrivial problem to solve.
-- Yeah.

92
00:03:58,759 --> 00:04:00,679
And then even once you start to understand it,

93
00:04:00,690 --> 00:04:02,460
you get back to some of our classic security

94
00:04:02,470 --> 00:04:04,830
paradigms where like look at the target breach,

95
00:04:05,020 --> 00:04:09,500
they had a SIM installed, they had all the data flowing into their SIM,

96
00:04:09,639 --> 00:04:12,279
but just having the data does not help you.

97
00:04:12,289 --> 00:04:15,389
It was one log, you know, of tens of thousands and it was missed.

98
00:04:15,720 --> 00:04:18,630
Uh And so there's really the shift and this is something we experienced

99
00:04:18,899 --> 00:04:20,920
in other things we did at Twitter, which was

100
00:04:21,029 --> 00:04:22,149
automate everything.

101
00:04:22,290 --> 00:04:25,790
If you're operating at a human scale, you don't, you don't have a chance.

102
00:04:26,100 --> 00:04:27,369
And so that's kind of one of the,

103
00:04:27,380 --> 00:04:29,260
the principles that we've taken to what we're doing is how

104
00:04:29,269 --> 00:04:32,260
do we remove as much as possible humans from the equation?

105
00:04:32,339 --> 00:04:33,399
-- That,
-- that's huge.

106
00:04:33,410 --> 00:04:36,649
I agree with that because I would, I, I know that everyone always says, you know,

107
00:04:36,660 --> 00:04:38,299
you humans can scale linearly,

108
00:04:38,309 --> 00:04:42,010
-- but I have suspicion they actually scale logarithmically, not
-- linearly,

109
00:04:42,040 --> 00:04:43,500
logarithmically bad.

110
00:04:45,089 --> 00:04:47,140
Right. And this is something I've had to explain to people.

111
00:04:47,149 --> 00:04:50,350
Like, I, it's funny, I actually had my devs come to me and they're like, oh, you know,

112
00:04:50,359 --> 00:04:52,059
Kurt, should we be monitoring all the CV S?

113
00:04:52,070 --> 00:04:53,880
Right. Because they kind of think of me as the CV guy

114
00:04:54,040 --> 00:04:58,100
and I was like, no, actually, no, we shouldn't. And they were quite shocked, right?

115
00:04:58,109 --> 00:05:00,690
Because, well, we have to monitor these, don't we? And I'm like, no, because

116
00:05:00,899 --> 00:05:03,619
how often do you guys update the software and pull new depths and they're like,

117
00:05:03,649 --> 00:05:05,000
you know, 23 times a week.

118
00:05:05,010 --> 00:05:08,070
-- And I'm like, ok, so like you're updating the software every two days.
-- Yeah.

119
00:05:08,519 --> 00:05:08,709
Yeah.

120
00:05:08,720 --> 00:05:09,489
And if your,

121
00:05:09,500 --> 00:05:15,579
if your patching policy is all issues are patched within max 72 hours average 30.

122
00:05:15,589 --> 00:05:18,829
Like, that's incredible. Like people aspire to that in their dreams.

123
00:05:18,910 --> 00:05:20,480
But man, then you get into that other,

124
00:05:20,489 --> 00:05:23,619
other kind of thing that's biting everybody was, which is,

125
00:05:23,910 --> 00:05:28,010
you know, we think about all these really crazy uh security exploits,

126
00:05:28,019 --> 00:05:29,709
these esoteric issues.

127
00:05:30,130 --> 00:05:30,829
But

128
00:05:31,079 --> 00:05:32,230
inventory management,

129
00:05:32,239 --> 00:05:35,410
like just knowing which machines and software you have to even patch them.

130
00:05:35,420 --> 00:05:37,760
Like your patch policy policy is amazing provided,

131
00:05:37,769 --> 00:05:39,890
you know about all the software to patch.

132
00:05:39,899 --> 00:05:42,929
Well, yeah, the Equifax breach, the classic example of that.

133
00:05:43,059 --> 00:05:43,790
Yeah, exactly.

134
00:05:43,799 --> 00:05:45,070
And that's what's so weird is you get people

135
00:05:45,079 --> 00:05:46,790
talking about all sorts of crazy new things.

136
00:05:46,799 --> 00:05:47,130
Like

137
00:05:47,450 --> 00:05:49,829
we just need to know what systems you have and,

138
00:05:50,200 --> 00:05:50,209
uh,

139
00:05:50,399 --> 00:05:52,519
and people are blown away. They're like, what do you mean?

140
00:05:52,529 --> 00:05:53,459
You don't have a good inventory?

141
00:05:53,470 --> 00:05:56,829
Like, nope, that's probably the dirty little secret of security and every sea

142
00:05:56,970 --> 00:05:57,709
knows it. It's,

143
00:05:57,959 --> 00:06:01,380
we don't really know where everything is and we wish we did and could.

144
00:06:01,390 --> 00:06:03,179
It's not very easy to do.

145
00:06:03,309 --> 00:06:06,220
This is actually one advantage I'm finding of, of

146
00:06:06,364 --> 00:06:10,084
sort of the monthly billing cycle requirements for all the other service things is

147
00:06:10,255 --> 00:06:12,875
we can actually track down like we can basically see, ok,

148
00:06:12,885 --> 00:06:14,654
money is going out to the service provider.

149
00:06:14,665 --> 00:06:16,434
I guess that means we're using it now.

150
00:06:17,084 --> 00:06:19,424
Whereas with, you know, hardware and software, somebody installs it,

151
00:06:19,434 --> 00:06:20,984
plops it on an IP and it's,

152
00:06:21,274 --> 00:06:22,424
it's a ghost.

153
00:06:22,734 --> 00:06:23,695
Yeah. Yeah, definitely.

154
00:06:23,815 --> 00:06:27,524
You know, back to the cloud space, the CASB vendors are really good at that, uh,

155
00:06:27,535 --> 00:06:29,355
that shadow it and reporting problem.

156
00:06:29,364 --> 00:06:30,505
It's a kind of

157
00:06:30,704 --> 00:06:30,714
a,

158
00:06:31,420 --> 00:06:33,140
it's an interesting space.

159
00:06:33,149 --> 00:06:38,339
That's the, uh, the, again, inventory management or what not of, of security,

160
00:06:38,350 --> 00:06:41,000
but it's a good one to have those players,

161
00:06:41,299 --> 00:06:44,399
you know, telling you which new services you signed up for is a, is a good,

162
00:06:44,410 --> 00:06:45,269
good play for them.

163
00:06:45,410 --> 00:06:47,739
So let, let me ask you kind of two things here.

164
00:06:47,750 --> 00:06:51,799
You just said CASB, which I'll have you to find for people who might not know, but then

165
00:06:52,130 --> 00:06:54,190
how is, what you're doing different from AC

166
00:06:54,290 --> 00:06:54,839
A SB?

167
00:06:55,239 --> 00:06:56,839
Oh, yeah. Yeah, great question. Yeah, I think

168
00:06:57,149 --> 00:06:57,179
at

169
00:06:57,290 --> 00:07:00,369
the uh or at the infancy of uh cloud security.

170
00:07:00,380 --> 00:07:03,820
So when you say even cloud security people will correctly say, well,

171
00:07:03,829 --> 00:07:07,730
do you mean is or pass or SAS or, you know?

172
00:07:07,839 --> 00:07:09,130
And so that's good. Um

173
00:07:09,320 --> 00:07:12,519
But yeah, CASB stands for cloud access security broker,

174
00:07:12,529 --> 00:07:14,489
as I kind of mentioned to you before, they were a,

175
00:07:14,500 --> 00:07:17,660
a space that was created at least in my opinion, you know,

176
00:07:17,670 --> 00:07:21,399
share your thoughts but was created in response to the

177
00:07:21,809 --> 00:07:26,500
uh explosion of, of cloud services and cloud platforms that companies could adopt.

178
00:07:26,600 --> 00:07:29,519
Employees would go and use their personal credit cards or maybe

179
00:07:29,529 --> 00:07:31,230
their work credit cards and just sign up for things,

180
00:07:31,239 --> 00:07:35,059
bypassing internal processes and then start expensing them back to the business.

181
00:07:35,070 --> 00:07:37,459
And the business said, wait a minute, what is all this stuff?

182
00:07:37,470 --> 00:07:39,329
You know, you can't just skip our procurement,

183
00:07:39,339 --> 00:07:40,989
you can't just skip security and governance.

184
00:07:41,070 --> 00:07:41,100
Yes,

185
00:07:41,320 --> 00:07:42,670
they can. And so

186
00:07:43,140 --> 00:07:45,934
exactly, exactly. They, they did it could.

187
00:07:46,125 --> 00:07:49,554
And so in response to that, you know, uh some vendors said, hey,

188
00:07:49,565 --> 00:07:53,244
we can put a device on your perimeter, we can monitor all these transactions.

189
00:07:53,505 --> 00:07:56,484
They've since combined perimeters with API S

190
00:07:56,815 --> 00:08:00,144
and, and they do a great job on, on that shadow it problem.

191
00:08:00,184 --> 00:08:04,255
Um They have of course expanded into um other spaces in

192
00:08:04,265 --> 00:08:07,325
cloud security but to your question of what's the difference is,

193
00:08:07,690 --> 00:08:09,600
is very much like talking about

194
00:08:09,839 --> 00:08:12,750
network security as a whole versus a very targeted

195
00:08:12,760 --> 00:08:15,619
solution that's finding a very specific and hard problem.

196
00:08:15,630 --> 00:08:16,920
And that's really where we are,

197
00:08:17,000 --> 00:08:22,769
we are going deep into this data security inside of SAS applications.

198
00:08:22,929 --> 00:08:24,510
And so we can find things that are

199
00:08:24,519 --> 00:08:27,959
very granular and very important to your organization.

200
00:08:27,970 --> 00:08:28,470
Like

201
00:08:28,640 --> 00:08:33,932
did this employee just share this board deck with their personal email account

202
00:08:34,072 --> 00:08:38,592
all of that context that connects those dots that says, oh, that is incredibly bad.

203
00:08:38,712 --> 00:08:42,773
That small needle in this massive haystack is incredibly important to us.

204
00:08:43,062 --> 00:08:45,612
And thank God you pulled that out very accurately.

205
00:08:45,882 --> 00:08:50,093
And so we go deep into what is the type of information, what is the sensitivity,

206
00:08:50,103 --> 00:08:51,973
what is the relationship graphs?

207
00:08:52,182 --> 00:08:55,372
So we can start to do all that analysis to find,

208
00:08:55,536 --> 00:08:58,385
find those problems surface them accurately and

209
00:08:58,395 --> 00:09:01,125
then have integration paths into your workflows.

210
00:09:01,245 --> 00:09:03,536
And and that's another interesting point because integration

211
00:09:03,546 --> 00:09:07,366
workflows goes back to the orchestration security space.

212
00:09:07,486 --> 00:09:10,796
And so I see these, these interesting trends where

213
00:09:11,166 --> 00:09:14,156
we have explosions of investment and technology in areas that

214
00:09:14,166 --> 00:09:17,106
people need solutions and some of them are workflow related,

215
00:09:17,116 --> 00:09:19,356
some of them are inventory related, some of them are

216
00:09:19,690 --> 00:09:22,080
uh almost deep specific problems.

217
00:09:22,090 --> 00:09:25,869
Um And so we're excited to focus on that, that new problem of

218
00:09:26,150 --> 00:09:28,270
who is collaborating with whom

219
00:09:28,640 --> 00:09:32,169
and what data is being exchanged and then bringing in the best of the other areas,

220
00:09:32,179 --> 00:09:36,270
which is how do you have intelligent workflow automation so that the system can work

221
00:09:36,440 --> 00:09:38,580
-- at the scale of a security program?
-- Nice.

222
00:09:38,590 --> 00:09:39,419
Now, now, does,

223
00:09:39,429 --> 00:09:43,059
do you guys integrate with other like SIM style solutions

224
00:09:43,070 --> 00:09:45,130
or are you kind of stand alone right now?

225
00:09:45,469 --> 00:09:48,140
Well, we're early, you know, we're like I said, we're one year in.

226
00:09:48,150 --> 00:09:51,429
So we're, we've got the uh the engine running, we can come in,

227
00:09:51,440 --> 00:09:52,809
walk in the door and give you,

228
00:09:53,099 --> 00:09:55,580
uh I would say a surprising amount of value.

229
00:09:55,590 --> 00:09:58,179
This is kind of an untapped green field and people are like, wait,

230
00:09:58,239 --> 00:09:59,979
who is sharing what you like?

231
00:09:59,989 --> 00:10:00,640
Oh my God.

232
00:10:00,809 --> 00:10:04,359
I can only imagine the first conversations you have

233
00:10:04,609 --> 00:10:05,200
where

234
00:10:05,359 --> 00:10:08,280
people don't even have a clue what's going on.

235
00:10:08,619 --> 00:10:11,440
And oh my goodness, it's got to be epic.

236
00:10:11,460 --> 00:10:15,760
It's exciting from our kind of security mind, exciting. It's really not as

237
00:10:15,979 --> 00:10:18,919
you know, I can see on the other side, you kind of have mixed emotions like, oh,

238
00:10:18,929 --> 00:10:20,070
this is, this is bad.

239
00:10:20,080 --> 00:10:24,239
I'm glad I know it. And from our side, like our job is to find these things. So success.

240
00:10:24,400 --> 00:10:26,239
-- Let's
-- shift gears a little bit.

241
00:10:26,250 --> 00:10:29,424
I've got a question that I've been thinking about a week knowing

242
00:10:29,434 --> 00:10:32,885
I get to talk to you is I feel like we're seeing

243
00:10:33,054 --> 00:10:34,854
this weird nexus point

244
00:10:35,015 --> 00:10:38,844
of privacy and data security and like GDPR

245
00:10:38,854 --> 00:10:40,965
comes into play here and there's companies worried about

246
00:10:40,974 --> 00:10:43,335
their secrets getting out of the internet and companies

247
00:10:43,344 --> 00:10:46,405
are uploading essentially confidential information to third parties,

248
00:10:46,414 --> 00:10:48,284
you know, like in Google Docs and things like that.

249
00:10:48,619 --> 00:10:51,140
And I'm just curious to get your opinion of your,

250
00:10:51,150 --> 00:10:54,760
your general thoughts of the data landscape and if you have any,

251
00:10:54,940 --> 00:10:57,460
I guess, insights into where you might think we're headed.

252
00:10:57,659 --> 00:11:00,359
Oh, yeah, that's a fantastic, fantastic question.

253
00:11:01,260 --> 00:11:07,840
Um I think it is so apropos because as I have talked about the change in security,

254
00:11:08,200 --> 00:11:12,219
um I think there's a massive shift from the way we used to think about security,

255
00:11:12,229 --> 00:11:13,219
which was

256
00:11:13,359 --> 00:11:18,380
perimeters, internal and external attacker, putting your layers of defenses,

257
00:11:18,530 --> 00:11:24,080
all of those things are fine and they're not wrong now, but they are not the focus.

258
00:11:24,090 --> 00:11:27,010
And I think everything is shifting towards what I

259
00:11:27,020 --> 00:11:30,059
like to call a data first security program.

260
00:11:30,270 --> 00:11:33,380
And this was something that I put into practice at

261
00:11:33,609 --> 00:11:34,450
Twitter.

262
00:11:34,890 --> 00:11:38,320
And the nexus of that thinking is

263
00:11:38,549 --> 00:11:41,450
why are we making these arbitrary boundaries where

264
00:11:41,460 --> 00:11:44,289
we think Attackers live like a perimeter?

265
00:11:44,520 --> 00:11:45,359
Because

266
00:11:45,719 --> 00:11:50,429
when you say we have a strong perimeter, but we kind of have a soft internal center,

267
00:11:50,440 --> 00:11:51,130
but that's OK.

268
00:11:51,140 --> 00:11:53,000
There's not really internal Attackers,

269
00:11:53,530 --> 00:11:55,429
one that's a false premise.

270
00:11:56,429 --> 00:11:59,260
So one, you've got just a false premise to begin with. Exactly.

271
00:11:59,270 --> 00:12:00,679
So the laughter correct.

272
00:12:01,090 --> 00:12:02,659
Um But two,

273
00:12:02,820 --> 00:12:05,789
the other thing you're saying is that if anyone breaches our perimeter,

274
00:12:05,799 --> 00:12:09,229
they're like one vulnerability or one exploit away from totally owning

275
00:12:09,239 --> 00:12:11,590
everything because we can't do anything once they're on the inside.

276
00:12:12,280 --> 00:12:16,210
And so if you combine that, that breakdown of that thinking with the new reality,

277
00:12:16,219 --> 00:12:19,909
which is, you know, business partners outsource workers,

278
00:12:20,119 --> 00:12:22,380
um interconnected systems,

279
00:12:22,390 --> 00:12:26,219
uh cloud use all of those things like the whole thing just totally falls apart.

280
00:12:26,530 --> 00:12:29,739
And so the shift has to be to data first security.

281
00:12:29,750 --> 00:12:31,869
And so what that means is wherever your most

282
00:12:31,880 --> 00:12:34,349
sensitive data lives that matters to your company,

283
00:12:34,359 --> 00:12:36,030
you start right there and you say,

284
00:12:36,140 --> 00:12:39,489
how do we know who's accessing that data in its current spot?

285
00:12:39,500 --> 00:12:41,989
And that's deep inside your internal network

286
00:12:42,000 --> 00:12:43,710
and you have internal services access in it

287
00:12:44,119 --> 00:12:46,039
like, all right. Well, we can figure out these services that access.

288
00:12:46,059 --> 00:12:48,919
Well, who's calling the services? How do you know if those are being abused?

289
00:12:49,090 --> 00:12:51,340
And so you work from concentric circles outward,

290
00:12:51,349 --> 00:12:54,940
so there's no need to spend tons of time on your perimeter with the next latest,

291
00:12:54,950 --> 00:12:57,599
next gen whatever if you can't tell me who

292
00:12:57,609 --> 00:12:59,929
internal to your company is even accessing the data.

293
00:13:00,479 --> 00:13:01,619
Uh And this is,

294
00:13:01,630 --> 00:13:05,690
this makes perfect sense with the shift in the overall landscape like privacy.

295
00:13:05,700 --> 00:13:06,840
Uh GDPR

296
00:13:07,080 --> 00:13:10,340
uh the expectations of on companies on how they

297
00:13:10,349 --> 00:13:14,330
protect data is rising to the level that is

298
00:13:14,690 --> 00:13:15,849
reasonable.

299
00:13:15,979 --> 00:13:17,739
We've been in a state for many years where

300
00:13:17,750 --> 00:13:19,489
it's unreasonable people are just like uh whatever,

301
00:13:19,500 --> 00:13:21,510
like please don't get breached generally,

302
00:13:21,929 --> 00:13:22,270
but

303
00:13:22,570 --> 00:13:24,390
now we're at the right spot

304
00:13:24,619 --> 00:13:28,309
now, we are saying you cannot have your data breach.

305
00:13:28,320 --> 00:13:30,570
You cannot not even know where your data is flowing.

306
00:13:30,580 --> 00:13:32,530
And that was one of the great things with GDPR is

307
00:13:32,690 --> 00:13:35,919
it forced companies to do an inventory management exercise of

308
00:13:36,119 --> 00:13:40,030
who actually touches your data. Do you even know where it is? And the answer to that?

309
00:13:40,039 --> 00:13:43,070
Just like the other inventory thing before was no people did not know.

310
00:13:43,659 --> 00:13:46,239
Uh And so this was another great forcing function. So

311
00:13:46,520 --> 00:13:49,979
I'm super excited about where we're going. It's hard, do not get me wrong.

312
00:13:49,989 --> 00:13:51,840
This is a hard thing for companies,

313
00:13:51,849 --> 00:13:54,419
but it's the right thing that lets us take technology to

314
00:13:54,429 --> 00:13:58,299
the next level and hopefully lets us use data responsibly.

315
00:13:58,309 --> 00:14:03,239
Uh I'm a big proponent of users having control of their data and being in charge.

316
00:14:03,419 --> 00:14:06,919
I think we're at a constant grappling point between how that plays out with,

317
00:14:06,929 --> 00:14:08,619
with companies and use of data.

318
00:14:08,630 --> 00:14:08,919
But

319
00:14:09,190 --> 00:14:11,880
our, our current forcing functions are, are the right ones that are good.

320
00:14:12,179 --> 00:14:12,390
Yeah.

321
00:14:12,400 --> 00:14:12,710
No, I mean,

322
00:14:12,719 --> 00:14:16,390
that's been the focus of a lot of my work at the CS A is you can give users privacy

323
00:14:16,400 --> 00:14:18,130
but how do we give them privacy but still let

324
00:14:18,140 --> 00:14:20,479
them use and control their data in a useful way,

325
00:14:20,489 --> 00:14:20,929
right?

326
00:14:20,940 --> 00:14:21,359
I mean,

327
00:14:21,710 --> 00:14:23,359
uh I deal with a lot of people in

328
00:14:23,369 --> 00:14:26,080
the financial industry and they can't access Google Drive,

329
00:14:26,090 --> 00:14:26,520
right?

330
00:14:26,530 --> 00:14:28,890
So because their security policy has been, well,

331
00:14:28,900 --> 00:14:30,500
we don't know how to lock this down and prevent

332
00:14:30,510 --> 00:14:32,989
you from downloading Lord knows what from it or putting,

333
00:14:33,169 --> 00:14:34,070
knows what into it.

334
00:14:34,359 --> 00:14:37,130
So we're just blocking drive.google.com.

335
00:14:37,299 --> 00:14:42,770
Yeah, and that's the perfect example of when security can't enable business,

336
00:14:42,780 --> 00:14:45,080
business disabled technology.

337
00:14:45,090 --> 00:14:48,559
And so security is an enabler and can unlock a lot of things.

338
00:14:48,739 --> 00:14:51,369
Um Your your comment about privacy choices

339
00:14:51,380 --> 00:14:54,010
in different um platforms is really interesting

340
00:14:54,260 --> 00:14:59,400
that that was like an instant throwback to one of the big lessons I saw from my time at

341
00:14:59,500 --> 00:14:59,890
Mozilla.

342
00:15:00,280 --> 00:15:01,289
Um and this is,

343
00:15:01,950 --> 00:15:04,650
it's interesting as we secure data and we think about

344
00:15:04,659 --> 00:15:09,869
options for the more uh informed security or privacy aware individuals

345
00:15:10,239 --> 00:15:14,869
because what I found and learned from from my time at Mozilla was that

346
00:15:15,390 --> 00:15:17,349
defaults rule the world

347
00:15:18,539 --> 00:15:22,250
and you can make this sort of skeleton claim

348
00:15:22,260 --> 00:15:25,429
that you support user privacy by having these options.

349
00:15:25,440 --> 00:15:27,119
But if they default off,

350
00:15:27,330 --> 00:15:29,599
then what you're doing is saying the people that really want

351
00:15:29,609 --> 00:15:31,489
it can go and figure out how to use it maybe

352
00:15:31,989 --> 00:15:33,169
and then the rest of you,

353
00:15:33,179 --> 00:15:35,729
I'm going to mine all of your data and you're going to

354
00:15:35,739 --> 00:15:38,309
think I'm doing you a favor of privacy because of my announcement,

355
00:15:38,320 --> 00:15:39,179
but really

356
00:15:39,349 --> 00:15:40,690
you have not figured this out.

357
00:15:40,820 --> 00:15:42,289
And so we saw that the defaults, you know,

358
00:15:42,299 --> 00:15:45,200
99% of the time defaults are what's what's chosen.

359
00:15:45,380 --> 00:15:46,049
And so we,

360
00:15:46,059 --> 00:15:50,250
we really have to avoid this trap of thinking that empowering people with choice

361
00:15:51,025 --> 00:15:51,974
is actually

362
00:15:52,224 --> 00:15:54,174
letting them make an informed choice.

363
00:15:54,184 --> 00:15:57,174
Um Because again, the defaults you pick are what matters the most.

364
00:15:57,534 --> 00:16:00,244
I think you're entirely correct with the defaults because part of it too is

365
00:16:00,385 --> 00:16:05,224
understanding these privacy controls as well is incredibly nontrivial,

366
00:16:05,234 --> 00:16:08,614
even for people in the industry, let alone, you know, my parents or my kids.

367
00:16:09,070 --> 00:16:11,210
So I watched your

368
00:16:11,469 --> 00:16:11,729
Abs

369
00:16:11,919 --> 00:16:16,530
se keynote in 2018, I'll put a link in the show notes and you at one point

370
00:16:16,809 --> 00:16:19,859
point out that it should be easier to do the

371
00:16:19,869 --> 00:16:22,090
secure thing than it is to do the insecure thing,

372
00:16:22,099 --> 00:16:24,650
which I think is basically the opposite of

373
00:16:24,909 --> 00:16:27,049
most things today, right?

374
00:16:27,299 --> 00:16:27,750
Yeah,

375
00:16:28,010 --> 00:16:29,280
I enjoyed that talk.

376
00:16:29,289 --> 00:16:31,719
Thanks for watching it and thanks for anyone who goes and watches it.

377
00:16:31,880 --> 00:16:32,880
It's great talk, man.

378
00:16:33,090 --> 00:16:33,440
Yeah.

379
00:16:33,450 --> 00:16:37,140
You know, the longer I've been in security, you know, I started very,

380
00:16:37,150 --> 00:16:38,830
very deep and very technical

381
00:16:39,090 --> 00:16:40,599
and the longer that I'm in it,

382
00:16:40,940 --> 00:16:43,500
the more I look at other fields like

383
00:16:44,000 --> 00:16:46,609
economics and psychology and

384
00:16:46,940 --> 00:16:49,309
human behavior analysis and think about,

385
00:16:49,429 --> 00:16:54,460
wow, these things have huge impacts on security. And so in that talk, I go into this,

386
00:16:55,039 --> 00:16:56,859
this, this notion of, you know,

387
00:16:56,869 --> 00:16:58,969
what I call academic security and I kind of do

388
00:16:58,979 --> 00:17:01,159
some air quotes which for lack of a better word

389
00:17:01,460 --> 00:17:05,790
like that doesn't buy us a lot if it isn't practically usable.

390
00:17:05,800 --> 00:17:08,118
And my example in the talk is like P GP,

391
00:17:08,250 --> 00:17:11,390
-- it is awesome on paper.
-- I've actually given up on

392
00:17:11,400 --> 00:17:11,550
it,

393
00:17:12,368 --> 00:17:12,949
sadly,

394
00:17:13,150 --> 00:17:16,079
-- after 20 years of trying.
-- Yeah. But on paper, it's amazing.

395
00:17:16,598 --> 00:17:19,680
You know, like there's, you can't fault it. It's totally right,

396
00:17:19,959 --> 00:17:24,719
but nobody uses it. It doesn't actually provide security to humans

397
00:17:24,959 --> 00:17:29,270
because it's not usable and sure there are other use cases, machine to machine,

398
00:17:29,280 --> 00:17:29,579
etcetera.

399
00:17:29,589 --> 00:17:29,910
But

400
00:17:30,400 --> 00:17:34,760
that really gets to me. And when I looked at a security program, you know,

401
00:17:34,959 --> 00:17:35,640
Mozilla and Twitter,

402
00:17:35,849 --> 00:17:41,140
it was really about how do we build these workflows, these paths and in many cases,

403
00:17:41,150 --> 00:17:42,750
these technologies that

404
00:17:43,150 --> 00:17:43,810
make it.

405
00:17:43,819 --> 00:17:46,869
So we're getting security to your point before

406
00:17:46,880 --> 00:17:49,510
security by default because that's the easy path.

407
00:17:49,969 --> 00:17:50,709
And

408
00:17:51,079 --> 00:17:54,739
I think the more we can move technology in a direction of creating

409
00:17:55,099 --> 00:17:57,900
uh reusable components or, you know,

410
00:17:57,910 --> 00:18:02,099
almost bricks that you can build a building um knowing that they have the,

411
00:18:02,130 --> 00:18:04,339
the security built in by default,

412
00:18:04,520 --> 00:18:08,160
that will let people scale and do things securely because it's just not

413
00:18:08,170 --> 00:18:12,800
possible to know all the different iterations of what you should be doing.

414
00:18:13,099 --> 00:18:14,709
And so if we look at that analogy, you know,

415
00:18:14,839 --> 00:18:18,780
you build a building out of bricks or, or whatever, but you know,

416
00:18:18,790 --> 00:18:22,520
each of those components are structurally sound because you bought them

417
00:18:22,530 --> 00:18:24,890
from the right people that have the right test in place,

418
00:18:24,900 --> 00:18:26,939
you don't have to go and retest your bricks

419
00:18:27,130 --> 00:18:28,160
each time you use it.

420
00:18:28,630 --> 00:18:31,859
Um So you just know if you use them in the right way, you get all their guarantees.

421
00:18:32,119 --> 00:18:33,810
And that's kind of the model we have to move to.

422
00:18:33,819 --> 00:18:37,869
That was actually something I did at Mozilla in my first couple of years there.

423
00:18:38,089 --> 00:18:40,540
I sat down with the uh the web team

424
00:18:40,839 --> 00:18:41,589
and said, look,

425
00:18:41,599 --> 00:18:44,420
let's look at your framework and there seems to be some

426
00:18:44,430 --> 00:18:46,479
recurring security problems that are

427
00:18:46,489 --> 00:18:48,319
happening some opportunities we're missing.

428
00:18:48,760 --> 00:18:53,510
Why don't we go in and flip it on its head so that we turn on all the,

429
00:18:53,520 --> 00:18:56,140
the different security settings that we know we need

430
00:18:56,310 --> 00:18:58,030
and have that be your standard build.

431
00:18:58,369 --> 00:19:01,069
And that was a great situation where I came to them

432
00:19:01,079 --> 00:19:05,030
as a security individual from almost a products perspective and said,

433
00:19:05,040 --> 00:19:06,750
here's the objectives we want to achieve.

434
00:19:07,020 --> 00:19:09,979
Those engineers said. All right. Well, we're the engineers, we're the best at this.

435
00:19:09,989 --> 00:19:12,369
Don't tell us how to do it. Just tell us what you want to accomplish.

436
00:19:12,780 --> 00:19:14,020
And together we,

437
00:19:14,140 --> 00:19:15,199
we flipped that

438
00:19:15,390 --> 00:19:16,329
and it worked great.

439
00:19:16,339 --> 00:19:18,989
Then they use this new building and out of the box, they get, you know,

440
00:19:19,000 --> 00:19:20,869
HSTS turned on or,

441
00:19:21,160 --> 00:19:24,469
um, the correct, uh, flags for cross site scripting protection, um,

442
00:19:24,479 --> 00:19:25,709
or whatever it was at the time.

443
00:19:25,719 --> 00:19:27,390
And that was great because when you build it,

444
00:19:27,604 --> 00:19:29,435
that turned out by default, it's a quick fix.

445
00:19:29,444 --> 00:19:33,555
If you have a bug that comes from that setting versus of course,

446
00:19:33,564 --> 00:19:36,665
everyone here knows if you go ahead and try and switch that afterwards,

447
00:19:36,675 --> 00:19:38,765
you have all sorts of cascading failures.

448
00:19:38,775 --> 00:19:40,025
To try and get back up to speed.

449
00:19:40,045 --> 00:19:44,944
So, yeah, this, this whole notion of we have to have the easy path, be the secure path

450
00:19:45,285 --> 00:19:46,905
and there's a lot of ways to get there.

451
00:19:46,915 --> 00:19:49,864
But if we're not aspiring to that, we're going to keep being in this,

452
00:19:49,875 --> 00:19:54,375
this spiral of security challenges, we'll say for lack of a better word.

453
00:19:54,385 --> 00:19:56,584
-- I think,
-- I think we just call it normal now. Right.

454
00:19:57,270 --> 00:19:59,569
Well, I have to say in general web browsers,

455
00:19:59,579 --> 00:20:02,160
especially Firefox and Chrome have gotten

456
00:20:02,449 --> 00:20:03,839
vastly

457
00:20:03,969 --> 00:20:05,189
more secure.

458
00:20:05,199 --> 00:20:10,000
And I mean, and a great example for Firefox is a few, I think it was two weeks ago,

459
00:20:10,010 --> 00:20:10,800
they had that little

460
00:20:11,010 --> 00:20:12,560
uh certificate, whoopsie

461
00:20:12,760 --> 00:20:15,760
um with the extensions, you know, being signed essentially, you know,

462
00:20:15,770 --> 00:20:16,560
Firefox used.

463
00:20:16,569 --> 00:20:20,479
Well, they've had extensions I think pretty much since almost day one or very early.

464
00:20:20,819 --> 00:20:24,550
And, you know, there were a lot of malicious extensions and so with the signing,

465
00:20:24,560 --> 00:20:25,310
they then, you know,

466
00:20:25,319 --> 00:20:27,560
were able to well essentially clean up

467
00:20:27,569 --> 00:20:30,160
the landscape very quickly and very effectively

468
00:20:30,270 --> 00:20:31,189
problem solved.

469
00:20:31,770 --> 00:20:33,880
Yeah, I mean, a signing

470
00:20:34,890 --> 00:20:39,280
to some degree identity or at least a culpability is kind of a huge, a

471
00:20:39,630 --> 00:20:42,119
huge issue in, in a marketplace.

472
00:20:42,439 --> 00:20:42,790
But what,

473
00:20:42,800 --> 00:20:46,510
what's really interesting though is we see the add on marketplace for Firefox,

474
00:20:46,520 --> 00:20:47,810
you know, the chrome extensions.

475
00:20:47,819 --> 00:20:51,650
And then we see of course, the apps in both I Os and Android

476
00:20:52,060 --> 00:20:53,589
and that whole dynamic is,

477
00:20:53,599 --> 00:20:57,959
is really interesting and I'm actually surprised we haven't had people dive in

478
00:20:58,359 --> 00:21:00,510
more and start to tackle it.

479
00:21:00,810 --> 00:21:05,949
I have this lingering fear that there's a ton of scary applications out there,

480
00:21:05,959 --> 00:21:07,790
maybe signed or otherwise.

481
00:21:07,800 --> 00:21:10,890
Um, not specifically in, in Firefox add ons, but,

482
00:21:11,109 --> 00:21:14,550
but when you look at the uh the marketplace, the Android marketplace

483
00:21:15,180 --> 00:21:19,280
and back to the, you know, looping back just a few topics ago,

484
00:21:19,290 --> 00:21:22,790
we were talking about user choice and

485
00:21:22,989 --> 00:21:25,030
informed, informed decision making.

486
00:21:25,250 --> 00:21:26,770
We see that playing out as,

487
00:21:26,780 --> 00:21:31,069
as a classic case study between the two permission models of android and I OS

488
00:21:31,469 --> 00:21:32,829
uh android. Um

489
00:21:32,959 --> 00:21:35,989
you know, had this very, here's all the permissions, you know,

490
00:21:36,000 --> 00:21:38,520
do you want to prove access to this access to that, et cetera?

491
00:21:38,530 --> 00:21:39,930
And I OS has the very

492
00:21:40,300 --> 00:21:44,510
uh we're gonna do all the work for you, um you know, install it or don't install it.

493
00:21:44,900 --> 00:21:49,900
Um And I think that's a really interesting journey to see which one of those works

494
00:21:50,180 --> 00:21:51,109
works better.

495
00:21:51,119 --> 00:21:54,459
And what are the outcomes and what do users even understand is, is going on?

496
00:21:54,500 --> 00:21:59,280
I, I would guess most people have no clue and they just click allow

497
00:21:59,569 --> 00:22:01,770
when you're asked because I mean, I see, I,

498
00:22:01,780 --> 00:22:04,010
I have an Android phone and I get that question all the time, you know,

499
00:22:04,020 --> 00:22:06,550
do you want to this app access your contacts?

500
00:22:06,560 --> 00:22:09,750
And of course, I, I always say no for the most part, but I,

501
00:22:09,760 --> 00:22:11,989
I guarantee someone like my parents,

502
00:22:12,239 --> 00:22:13,630
they don't even know what that means.

503
00:22:13,780 --> 00:22:16,760
Right. And I think that's one of those usability challenges.

504
00:22:16,829 --> 00:22:18,189
Well, the other problem too is, I mean,

505
00:22:18,199 --> 00:22:20,550
either the user is going to choose to use the app or not.

506
00:22:20,739 --> 00:22:24,300
And if they choose to use the app, I've noticed a bunch of apps now that, for example,

507
00:22:24,650 --> 00:22:26,430
uh I actually noticed this really, uh,

508
00:22:26,439 --> 00:22:28,989
last time my kids went and I went to the cabin and they,

509
00:22:29,000 --> 00:22:30,430
they had their ipads but no internet

510
00:22:30,890 --> 00:22:34,050
and some of the games were fine, like Minecraft and all that stuff.

511
00:22:34,060 --> 00:22:35,550
Kind of, the big titles were fine,

512
00:22:35,739 --> 00:22:37,670
but there was a lot of sort of dodgier games they'd

513
00:22:37,680 --> 00:22:40,859
installed that refused to run without access to their ad network,

514
00:22:42,369 --> 00:22:46,040
right? They just flat out refute, like I can't show you ads, I'm not going to run.

515
00:22:46,099 --> 00:22:50,180
All right. So we're, we're kind of approaching the end of our time here, Michael.

516
00:22:50,189 --> 00:22:52,420
So I'm curious if you want to

517
00:22:52,719 --> 00:22:55,699
fill this in with any parting thoughts or ideas or,

518
00:22:55,709 --> 00:22:59,380
or things you think we should watch out for coming or kind of

519
00:22:59,510 --> 00:23:01,959
it? It's your floor for the next couple of minutes. How about?

520
00:23:02,209 --> 00:23:02,619
Yeah.

521
00:23:02,849 --> 00:23:03,579
Yeah, I think,

522
00:23:03,790 --> 00:23:08,619
I think I'd love to, to bring a few of the threads for that we talked about before.

523
00:23:09,770 --> 00:23:09,790
I

524
00:23:09,949 --> 00:23:15,069
think that if we're not adopting this data first security approach,

525
00:23:15,079 --> 00:23:16,589
we're going to be in a lot of trouble.

526
00:23:16,599 --> 00:23:17,130
Um

527
00:23:17,760 --> 00:23:20,890
I, it isn't necessary as you think about

528
00:23:21,459 --> 00:23:24,069
regulations put put upon companies.

529
00:23:24,079 --> 00:23:26,670
Um you know, GDPR, California Data Privacy Act.

530
00:23:27,359 --> 00:23:32,699
Um But, but two that notion of the insider insider attacker,

531
00:23:32,969 --> 00:23:36,540
um definitely, you know, listeners remember that thing.

532
00:23:36,550 --> 00:23:38,569
If, if you're not thinking about insider Attackers,

533
00:23:38,579 --> 00:23:40,900
then you're just saying an external attacker can never get in

534
00:23:40,910 --> 00:23:42,579
which we know is going to fail at some point.

535
00:23:42,979 --> 00:23:46,400
But I was, I was, I don't know if happy is the word,

536
00:23:46,729 --> 00:23:51,189
but I noted in the uh the Verizon data breach report from this year

537
00:23:51,630 --> 00:23:53,170
that they say

538
00:23:53,589 --> 00:23:57,209
privileged misuse in air by insider attack.

539
00:23:57,219 --> 00:24:01,910
Uh Insider accounts accounted for 30% of breaches in one space.

540
00:24:02,300 --> 00:24:05,839
Uh That's a very compelling number, you know, a third.

541
00:24:06,520 --> 00:24:10,609
And what's, it's both insider that's interesting also misuse and,

542
00:24:11,239 --> 00:24:11,650
and this,

543
00:24:12,060 --> 00:24:14,189
this is a very interesting space because

544
00:24:14,609 --> 00:24:20,430
the spectrum of things we need to worry about on one end is the um expert attacker.

545
00:24:20,439 --> 00:24:22,810
And that's a very small percentage. Um You know, when, when

546
00:24:22,930 --> 00:24:25,469
we're saying advanced, you know, threats,

547
00:24:25,930 --> 00:24:27,910
um we don't need to focus on those first.

548
00:24:27,920 --> 00:24:31,069
Those are the toughest of the tough and they're going to take the easiest path.

549
00:24:31,500 --> 00:24:35,449
So, but move back on that spectrum and say, all right, there's casual malice,

550
00:24:35,459 --> 00:24:37,349
there's poor risk decision making

551
00:24:37,500 --> 00:24:39,180
and there's just mistakes.

552
00:24:39,449 --> 00:24:43,280
And so we have to think about that entire spectrum.

553
00:24:43,650 --> 00:24:48,260
Um both focusing on insiders um and thinking about outsiders.

554
00:24:48,390 --> 00:24:52,949
But I think if we pull that together while we think about data for security.

555
00:24:52,979 --> 00:24:56,439
I think that's the way to go and of course, you can Sprinkle in some other nuggets.

556
00:24:56,449 --> 00:25:00,270
We talked about like, uh, secure by default and easy to use,

557
00:25:00,589 --> 00:25:02,869
but maybe not, let's not get ahead of ourselves.

558
00:25:03,030 --> 00:25:06,260
But, um, but I think those are some really important takeaways. And,

559
00:25:06,989 --> 00:25:09,959
uh, you know, if we don't start to think about things in that, in that way,

560
00:25:09,969 --> 00:25:13,109
you run into the risk of what I've seen in other security programs, which is,

561
00:25:13,420 --> 00:25:16,930
you know, you're running around hair on fire. What's the latest headline?

562
00:25:16,939 --> 00:25:18,260
What random, you know,

563
00:25:18,599 --> 00:25:20,170
uh Zero Day came out

564
00:25:20,369 --> 00:25:22,780
and does all of that really matter more than,

565
00:25:22,920 --> 00:25:26,180
I don't even know who touches our most sensitive data every single day.

566
00:25:26,609 --> 00:25:29,150
And so you really got to put it in perspective, which is hard,

567
00:25:29,160 --> 00:25:31,729
it's hard as a security person to do that.

568
00:25:31,800 --> 00:25:33,630
I think that's the way forward through our

569
00:25:33,640 --> 00:25:36,569
very conscious decision making on our strategies around.

570
00:25:36,579 --> 00:25:38,750
What is our plan for data security?

571
00:25:39,010 --> 00:25:40,670
That is an awesome ending.

572
00:25:40,910 --> 00:25:40,959
I,

573
00:25:41,089 --> 00:25:43,780
I love your data for security approach.

574
00:25:43,790 --> 00:25:46,229
That is I'm totally stealing that and I'm going to use

575
00:25:46,449 --> 00:25:48,339
in the future because it's awesome.

576
00:25:48,599 --> 00:25:48,930
I like it.

577
00:25:49,189 --> 00:25:50,160
Wonderful.

578
00:25:50,270 --> 00:25:52,959
Well, thank you so much, man. Thank you, Michael. Thank you, Kurt.

579
00:25:52,969 --> 00:25:55,109
Thank you everyone for listening. This has been

580
00:25:55,359 --> 00:25:57,839
one of my favorite episodes. I think it's been awesome.

581
00:25:57,930 --> 00:26:01,939
So, yeah, I guess for the listeners you can go to open source security podcast.com.

582
00:26:01,949 --> 00:26:03,010
Hit up the show notes.

583
00:26:03,180 --> 00:26:05,900
You can use the Pound Os S podcast hashtag

584
00:26:06,199 --> 00:26:09,420
to hit us up on social media. Uh, give Michael a follow on Twitter.

585
00:26:09,430 --> 00:26:11,140
His, his tweets are super interesting.

586
00:26:11,150 --> 00:26:15,630
-- Yeah, I guess gentlemen have fabulous rest of your days.
-- Thanks so much.

587
00:26:15,640 --> 00:26:17,380
-- This is wonderful.
-- Thanks, everybody.

588
00:26:17,430 --> 00:26:19,030
Awesome. Thank you, everyone. Bye bye.