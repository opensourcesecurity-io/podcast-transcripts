0
00:00:05,320 --> 00:00:10,020
Hello and welcome to the open source Security podcast episode 152 with myself,

1
00:00:10,029 --> 00:00:11,609
Kurt Siegfried and my partner on Thought Crime.

2
00:00:11,619 --> 00:00:12,350
Josh Bresser.

3
00:00:12,970 --> 00:00:13,909
-- What's
-- new Kurt?

4
00:00:13,920 --> 00:00:17,729
Well, uh, I guess apparently Tavis Ormandy hurt some people's feelings

5
00:00:17,889 --> 00:00:21,059
now. He ruined the world again, didn't he? And Tavis?

6
00:00:21,389 --> 00:00:24,500
Yeah, he's, uh, he tends to be the bearer of bad news for a lot of people.

7
00:00:24,520 --> 00:00:27,959
-- Yes,
-- I would not look forward to an email from Tavis

8
00:00:28,170 --> 00:00:31,790
or my all time favorite when Tavis tweets out on like a Friday.

9
00:00:31,799 --> 00:00:36,110
Hey, can somebody from Company X, please contact me like now? Right.

10
00:00:37,240 --> 00:00:42,240
-- You know, it's like, ah, crap,
-- fire up the coffee pot and cancel your weekend plans.

11
00:00:42,250 --> 00:00:43,580
Something's up,

12
00:00:43,689 --> 00:00:46,259
you know? And what drives me nuts is that everybody,

13
00:00:46,400 --> 00:00:50,700
just everybody. It's better than it used to be 20 years ago.

14
00:00:50,709 --> 00:00:53,979
But still so many people want to shoot the messenger and it's like, all right,

15
00:00:53,990 --> 00:00:57,369
why don't, why don't we explain what happened first before we start

16
00:00:57,659 --> 00:01:04,339
-- going into our meta stories about whatever?
-- So, Tavis Ormandy found a security bug.

17
00:01:04,430 --> 00:01:05,870
Just the one. Right.

18
00:01:06,559 --> 00:01:10,849
Well, and he's found lots of security bugs like on the order of over somewhere,

19
00:01:10,860 --> 00:01:14,919
if it's probably somewhere between 100 and 1000 at this point that I'm aware of a

20
00:01:15,120 --> 00:01:15,510
lot.

21
00:01:15,620 --> 00:01:19,959
Yeah. Anyways. And so, uh, he works for Google Project Zero Day

22
00:01:20,180 --> 00:01:24,050
and they have a 90 day disclosure policy, which to be fair is,

23
00:01:24,059 --> 00:01:25,250
I think that's pretty reasonable.

24
00:01:25,260 --> 00:01:27,010
I mean, 90 days either you can

25
00:01:27,379 --> 00:01:30,449
to put it bluntly, either you can fix it in 90 days or you can't.

26
00:01:31,029 --> 00:01:32,169
Right. Exactly.

27
00:01:32,180 --> 00:01:37,750
It really, like 90 days is not nine minutes, nine hours or nine days, it's 90 days,

28
00:01:37,760 --> 00:01:38,440
it's a quarter.

29
00:01:38,680 --> 00:01:41,260
And if you can't get it fixed in 90 days, chances are,

30
00:01:41,830 --> 00:01:44,589
well then, ok, it's just not going to happen any time soon.

31
00:01:44,669 --> 00:01:47,430
And so their disclosure policy is 90 days firm.

32
00:01:47,610 --> 00:01:48,529
Right. That's it.

33
00:01:48,800 --> 00:01:53,849
So, you know, Tavis finds a security flaw in windows, tells Microsoft, you know,

34
00:01:53,980 --> 00:01:56,940
and, and when we say Tavis finds a security flaw and tell somebody we're not like,

35
00:01:56,949 --> 00:01:59,790
hey, I found this, like, weird behavior in your program and like,

36
00:01:59,800 --> 00:02:00,529
I think it's a problem.

37
00:02:00,540 --> 00:02:01,660
No, like Tavis is like,

38
00:02:01,779 --> 00:02:05,330
here's like a nice big report with exploit code and like,

39
00:02:05,339 --> 00:02:06,680
everything you need to know.

40
00:02:06,760 --> 00:02:10,570
And I'll be blunt, Tavis probably understands that bit of code better than

41
00:02:11,119 --> 00:02:14,248
pretty much everybody at Microsoft other than maybe the person who wrote it

42
00:02:14,548 --> 00:02:18,819
anyway. So he told Microsoft and Microsoft took the report and then Microsoft

43
00:02:19,309 --> 00:02:22,949
didn't fix it. And then Tavis released on day 91 as

44
00:02:23,119 --> 00:02:27,149
well. He said he would. Yeah. And a whole bunch of people on Twitter got angry at him.

45
00:02:27,839 --> 00:02:28,479
Uh Well,

46
00:02:28,490 --> 00:02:30,820
so we should also clarify the security issue in

47
00:02:30,830 --> 00:02:35,100
question lets you remotely crash windows is my understanding,

48
00:02:35,110 --> 00:02:37,039
which is of course a non

49
00:02:37,360 --> 00:02:39,179
ideal thing to be happening.

50
00:02:39,389 --> 00:02:41,820
You know what it reminds me of is

51
00:02:42,179 --> 00:02:47,160
this is kind of how we got in the responsible disclosure situation in the first place

52
00:02:47,169 --> 00:02:48,919
where there's all these people that think I'm

53
00:02:48,929 --> 00:02:51,759
special and I should get magic treatment and

54
00:02:51,899 --> 00:02:55,440
I don't like what you're doing because I'm not in a situation to deal with this.

55
00:02:55,449 --> 00:02:57,229
So I don't like you

56
00:02:58,014 --> 00:03:01,835
about it and then sit on it until I'm ready to do something about

57
00:03:01,955 --> 00:03:01,975
it.

58
00:03:02,304 --> 00:03:03,494
Right. Right.

59
00:03:03,505 --> 00:03:05,024
And now I would like to point out I

60
00:03:05,035 --> 00:03:08,755
have not seen any negative anything coming from Microsoft.

61
00:03:08,764 --> 00:03:10,755
I have a suspicion. They,

62
00:03:11,014 --> 00:03:13,794
they, they're going to own this one the way they should.

63
00:03:14,145 --> 00:03:17,255
It's been like whinging on Twitter from various

64
00:03:17,264 --> 00:03:20,615
other like news organizations and people complaining that,

65
00:03:20,625 --> 00:03:22,235
oh, Tavis broke the world again.

66
00:03:22,244 --> 00:03:23,395
Well, here's the simple truth.

67
00:03:23,809 --> 00:03:26,669
The majority of people complaining have never worked security response,

68
00:03:26,679 --> 00:03:28,649
let alone security response at a vendor

69
00:03:29,360 --> 00:03:30,110
because

70
00:03:30,289 --> 00:03:32,589
you and I have worked security response at a vendor.

71
00:03:32,699 --> 00:03:37,839
I still do. Well, yeah, you still do. But that world is only a few 1000 people. Really?

72
00:03:38,059 --> 00:03:39,149
Yeah, probably. Right.

73
00:03:39,160 --> 00:03:41,979
Like, realistically I'm talking about public facing products.

74
00:03:42,080 --> 00:03:42,270
You know,

75
00:03:42,279 --> 00:03:43,910
there's a lot of people doing software

76
00:03:43,919 --> 00:03:46,360
security response for internal software services,

77
00:03:46,369 --> 00:03:46,789
whatever.

78
00:03:47,039 --> 00:03:50,699
But I'm talking about, you know, public facing products that are widely sold, like,

79
00:03:50,710 --> 00:03:52,089
say windows or, you know,

80
00:03:52,660 --> 00:03:53,770
a web browser or something.

81
00:03:53,779 --> 00:03:56,500
There's only if I had to guess I'd say between

82
00:03:56,529 --> 00:03:59,059
1000 and 10,000 people really doing that kind of work.

83
00:03:59,350 --> 00:04:01,419
Probably. I think that seems fair.

84
00:04:01,759 --> 00:04:05,820
-- So
-- to put it bluntly, those people also are too busy to complain on Twitter.

85
00:04:08,940 --> 00:04:14,009
Uh Well, not exactly. I mean, you have to remember there was a day

86
00:04:14,259 --> 00:04:17,519
when a 90 day disclosure policy was ridiculous.

87
00:04:17,529 --> 00:04:21,399
And in fact, I just not long ago finished the cult of the dead cow book

88
00:04:21,769 --> 00:04:24,119
and which I'll put a link in the show notes, everyone should read it.

89
00:04:24,130 --> 00:04:25,119
It's very, very good.

90
00:04:25,500 --> 00:04:29,130
And what it, it part of the book they talk about at stake

91
00:04:29,489 --> 00:04:32,329
and that was the loft guys kind of

92
00:04:32,549 --> 00:04:35,119
cashing in. I don't know what we want to call it.

93
00:04:35,130 --> 00:04:37,790
No, I wouldn't say cashing in so much as going professional.

94
00:04:37,799 --> 00:04:40,079
Uh that's for OK, going professional. I like that.

95
00:04:40,260 --> 00:04:42,440
So basically this talks about

96
00:04:42,899 --> 00:04:45,029
how the at stake guys

97
00:04:45,160 --> 00:04:45,480
they,

98
00:04:45,489 --> 00:04:51,239
they held firm to a 90 day disclosure period and that back then that was ridiculous.

99
00:04:51,519 --> 00:04:52,920
Like nobody did that.

100
00:04:53,470 --> 00:04:55,809
And you figure now, I mean,

101
00:04:56,149 --> 00:04:59,049
that's how it works. Google Project Zero has adopted it.

102
00:04:59,059 --> 00:05:02,649
They do 90 days and even when they started it, I remember it was

103
00:05:03,390 --> 00:05:06,329
and it was almost slightly controversial with a lot of people.

104
00:05:06,489 --> 00:05:07,880
And I feel like

105
00:05:08,010 --> 00:05:10,829
-- we've reached a point, it's fairly acceptable, but
-- there aren't any

106
00:05:11,000 --> 00:05:11,230
deadline

107
00:05:11,940 --> 00:05:12,640
controversial,

108
00:05:13,769 --> 00:05:16,839
realistically, any deadline would have been controversial.

109
00:05:16,850 --> 00:05:19,190
You know, what this reminds me of is when you say

110
00:05:19,440 --> 00:05:22,170
no to a child and then are firm about it.

111
00:05:22,850 --> 00:05:26,670
And the first few times that child is not happy, especially

112
00:05:26,850 --> 00:05:28,390
how do I put this nicely.

113
00:05:28,399 --> 00:05:32,070
There's a lot of my experience, there's a lot of Children who have not been

114
00:05:32,170 --> 00:05:34,510
given firm boundaries that are then enforced.

115
00:05:34,839 --> 00:05:37,230
And I'll be blunt, I, I treat my Children,

116
00:05:37,559 --> 00:05:40,649
you know, firm but fair and quite frankly, you know, if I,

117
00:05:40,660 --> 00:05:43,390
if I make an agreement with them, like we're going for ice cream,

118
00:05:43,399 --> 00:05:47,140
then we're going for ice cream or if you do this, then there will be consequent sex.

119
00:05:47,540 --> 00:05:48,880
Then there's consequent sex,

120
00:05:49,040 --> 00:05:50,200
end of story, even if

121
00:05:50,410 --> 00:05:53,100
you know, and there's been times where I don't want to leave. I want to stay here.

122
00:05:53,109 --> 00:05:57,019
-- But oh no, we did some behavior. So we're leaving man. This sucks.
-- I know.

123
00:05:57,049 --> 00:05:59,760
And it's funny to you, you say that because

124
00:06:00,190 --> 00:06:02,309
I, I'm, I'm very similar.

125
00:06:02,329 --> 00:06:07,239
I would say we have, I would guess very similar parenting styles in that regard. And

126
00:06:07,660 --> 00:06:08,200
we were,

127
00:06:08,209 --> 00:06:10,059
we were somewhere and there was this parent

128
00:06:10,070 --> 00:06:11,660
like playing on their phone and their kid was

129
00:06:11,670 --> 00:06:13,399
doing something ridiculous and they're like yelling at

130
00:06:13,410 --> 00:06:15,160
their kid without even looking up from the phone

131
00:06:15,660 --> 00:06:19,470
and my daughter looks at me and goes, that boy isn't going to listen to her, is he?

132
00:06:19,480 --> 00:06:20,579
I was like, no,

133
00:06:21,369 --> 00:06:23,079
not even a little bit.

134
00:06:23,859 --> 00:06:24,380
Yeah.

135
00:06:25,179 --> 00:06:25,410
And

136
00:06:25,899 --> 00:06:27,880
you're, you're absolutely correct.

137
00:06:27,890 --> 00:06:30,640
There is, there's something to be said about that and I think

138
00:06:30,799 --> 00:06:32,760
for disclosure,

139
00:06:33,010 --> 00:06:36,230
it was very much an instance of the researchers

140
00:06:36,239 --> 00:06:38,920
had to train the vendors back in the day

141
00:06:39,239 --> 00:06:44,500
-- and it wasn't easy and they were demonized. I mean,
-- it's still a problem

142
00:06:44,829 --> 00:06:50,100
with some people, but I think the major vendors for the most part understand this.

143
00:06:50,239 --> 00:06:53,529
Well, the, the reality that always gets me is this shoot the messenger thing.

144
00:06:53,540 --> 00:06:53,920
It's like

145
00:06:54,250 --> 00:06:56,450
number one, Tavis Ormandy didn't create the flaw,

146
00:06:56,970 --> 00:07:00,670
right. As far as I know, he didn't write new software for Microsoft Ever.

147
00:07:00,730 --> 00:07:01,750
And number two,

148
00:07:02,089 --> 00:07:03,359
it's not like Tavis Army

149
00:07:03,540 --> 00:07:05,649
is some magic space alien with magic

150
00:07:05,839 --> 00:07:09,510
-- alien technology to find security flaws.
-- You can't prove that.

151
00:07:09,519 --> 00:07:13,429
And I think the evidence would suggest that's a very real possibility.

152
00:07:13,440 --> 00:07:15,579
I've seen him eat a burrito. So I know he eats,

153
00:07:16,470 --> 00:07:17,299
we,

154
00:07:17,450 --> 00:07:20,179
we don't know, aliens probably have to eat

155
00:07:21,190 --> 00:07:21,720
but

156
00:07:21,929 --> 00:07:23,970
to put it bluntly, if Tavis can find it,

157
00:07:24,309 --> 00:07:27,329
then it's not outside the realm of possibility, somebody else can find it. And

158
00:07:27,570 --> 00:07:31,609
that's the reality. A lot of these security flaws when you, when you start looking

159
00:07:32,339 --> 00:07:33,619
or you know what to look for.

160
00:07:33,630 --> 00:07:36,269
Like, remember specter and meltdown and, oh my God,

161
00:07:36,279 --> 00:07:38,839
it's the end of the world we found hardware flaws and then guess what happened?

162
00:07:39,149 --> 00:07:41,510
A whole bunch of people found a whole bunch more hardware flaws

163
00:07:41,739 --> 00:07:44,480
and they're, they're going to keep finding them till time ends.

164
00:07:44,489 --> 00:07:46,040
I mean, that's the reality of it

165
00:07:46,200 --> 00:07:47,119
-- because
-- they're looking now.

166
00:07:47,299 --> 00:07:47,799
And

167
00:07:47,950 --> 00:07:48,640
so

168
00:07:48,920 --> 00:07:49,230
I

169
00:07:49,345 --> 00:07:51,005
look at this and, and, yeah, my,

170
00:07:51,015 --> 00:07:53,005
my experience of Microsoft is they're probably a little

171
00:07:53,015 --> 00:07:54,554
bit annoyed but more in the sense of like,

172
00:07:54,575 --> 00:07:57,375
uh, this again, not like, oh my God, it's the end of the world.

173
00:07:57,385 --> 00:07:59,635
Well, look, they didn't fix it

174
00:07:59,774 --> 00:08:04,065
well, and in fairness like this flaw while I would say highly annoying,

175
00:08:04,075 --> 00:08:05,994
like it is a denial service in the crypto thing.

176
00:08:06,005 --> 00:08:06,274
That

177
00:08:06,670 --> 00:08:08,890
means it's actually quite exploitable, right?

178
00:08:08,899 --> 00:08:10,959
Like I can send you basically mangled email and then your,

179
00:08:10,970 --> 00:08:13,369
your email client like poops itself basically.

180
00:08:13,380 --> 00:08:17,369
So, you know, most local denial services bluntly boil down to, ok,

181
00:08:17,380 --> 00:08:19,910
don't open that file again, like stop doing that.

182
00:08:19,920 --> 00:08:21,130
Right. Exactly.

183
00:08:21,140 --> 00:08:25,779
Whereas this is a little different in that you can remotely trigger it reasonably

184
00:08:26,149 --> 00:08:26,679
easily.

185
00:08:26,690 --> 00:08:28,890
I'm going to say, you know, through email or some other,

186
00:08:28,899 --> 00:08:32,630
some other well processing certificates from external

187
00:08:32,950 --> 00:08:36,729
-- entities is a pretty normal thing to do on a, on a machine. It,
-- it, it is.

188
00:08:36,739 --> 00:08:42,109
But so here's the way this is my guess of how this went down and this is me speculating,

189
00:08:42,119 --> 00:08:44,400
just putting on my, I've been doing this a long time.

190
00:08:44,409 --> 00:08:44,869
Hat

191
00:08:45,239 --> 00:08:49,250
Tavis tells Microsoft Microsoft probably did the investigation

192
00:08:49,479 --> 00:08:53,109
and it sounds like they told him kind of at the last minute we're not going to fix this.

193
00:08:53,119 --> 00:08:57,010
And my guess is they looked at the resources they had

194
00:08:57,030 --> 00:08:59,450
and the things that had to happen for the next patch.

195
00:09:00,010 --> 00:09:02,500
Well, well, I guess the next couple because they do this once a month,

196
00:09:02,510 --> 00:09:04,130
but at the same time I understand that

197
00:09:04,239 --> 00:09:07,590
rolling a patch into windows is going to take more than 30 days.

198
00:09:07,599 --> 00:09:09,260
And so they probably said we,

199
00:09:09,270 --> 00:09:12,909
we this isn't important enough to juggle this other work.

200
00:09:12,919 --> 00:09:15,169
So we're just going to push it off and oh, hey, Tavis,

201
00:09:15,179 --> 00:09:17,070
it's going to take us longer than 90 days.

202
00:09:17,080 --> 00:09:20,039
They knew what would happen, they knew this was going to happen. And if they were

203
00:09:20,289 --> 00:09:23,309
considerably concerned with this, they would have done something

204
00:09:23,450 --> 00:09:28,390
to either fix it or even ask politely for an extension or whatever. Right. So

205
00:09:28,520 --> 00:09:33,070
now at the same time, I have a suspicion if this was like a remote code execution flaw,

206
00:09:33,450 --> 00:09:35,309
they would have stopped the presses, right?

207
00:09:35,320 --> 00:09:38,109
And you're going to basically wrangle up everyone you need

208
00:09:38,119 --> 00:09:39,830
to and you're going to get this done right away.

209
00:09:39,840 --> 00:09:40,510
And so

210
00:09:40,820 --> 00:09:45,659
I think this is something else. People often forget in a lot of these situations is

211
00:09:45,780 --> 00:09:47,869
the, the severity of the issue

212
00:09:48,400 --> 00:09:50,340
has a lot of bearing on how,

213
00:09:50,440 --> 00:09:52,559
how much resources you're going to put into it.

214
00:09:52,570 --> 00:09:54,950
And in this case, I mean, let's face it it's not that bad.

215
00:09:55,289 --> 00:09:57,280
It's actually a little more subtle than that too because you

216
00:09:57,289 --> 00:10:00,750
could put in just enough resources to fix just that flaw or

217
00:10:01,080 --> 00:10:04,039
you could say, look, this isn't super critical. It can wait a bit.

218
00:10:04,280 --> 00:10:07,869
Let's actually free up a bunch of resources, not just to fix this flaw,

219
00:10:07,909 --> 00:10:12,349
but to actually audit this code and give it a, you know, go over it because, ok,

220
00:10:12,359 --> 00:10:13,109
no offense.

221
00:10:13,130 --> 00:10:14,669
If there's one bug in the code,

222
00:10:15,489 --> 00:10:16,710
there's probably more bugs.

223
00:10:16,950 --> 00:10:21,010
And my suspicion as well is that a lot of this code is probably

224
00:10:21,020 --> 00:10:24,820
old because I bet that's the kind of code you don't touch very often.

225
00:10:25,619 --> 00:10:30,070
And this, I mean, this is something Microsoft talked about at length with their,

226
00:10:30,080 --> 00:10:34,140
the beginning of the SDLC program they created many, many years ago

227
00:10:34,429 --> 00:10:38,760
is when any time they found a flaw in a function,

228
00:10:38,770 --> 00:10:42,419
they would typically rewrite the whole function with that exact assumption is

229
00:10:42,530 --> 00:10:46,409
there's probably other problems in here. Let's rewrite it using

230
00:10:46,609 --> 00:10:51,469
our, our new secure process with our reviews and everything else just to make sure

231
00:10:51,650 --> 00:10:54,270
there's nothing else here because in many instances,

232
00:10:54,570 --> 00:10:58,280
like looking for bugs is hard. Rewriting a function is easy.

233
00:10:58,289 --> 00:10:59,669
Well, and not just that, but

234
00:10:59,780 --> 00:11:04,130
I mean, you got to assume we've learned over the last 5, 1015, 20 years,

235
00:11:04,479 --> 00:11:06,619
a lot about what these functions are doing.

236
00:11:06,630 --> 00:11:08,700
There's performance improvements you can do,

237
00:11:08,840 --> 00:11:11,840
you can make, you can maybe add some functionality you know,

238
00:11:11,849 --> 00:11:14,299
you can just generally make it a whole lot better.

239
00:11:15,039 --> 00:11:16,820
You know, like I'm always reminded of Python, right?

240
00:11:16,830 --> 00:11:19,340
Python has a whole bunch of standard libraries

241
00:11:20,020 --> 00:11:23,969
and for example, like if you want to do a web request, you've got, oh God,

242
00:11:23,979 --> 00:11:24,469
what are they on now?

243
00:11:24,479 --> 00:11:28,299
Like the fifth or sixth version of it? There's like URL lib http lib,

244
00:11:28,650 --> 00:11:31,729
you know what I mean? Like they keep kind of writing a

245
00:11:31,989 --> 00:11:36,619
new one and they tend to layer on top and then add a bunch of new functionality and

246
00:11:36,799 --> 00:11:40,510
to put it bluntly, the latest one is generally the one you want to be using.

247
00:11:40,520 --> 00:11:41,659
It's so much better.

248
00:11:42,000 --> 00:11:45,299
Yeah. Yeah, because they take care of a lot of the nonsense for you.

249
00:11:45,900 --> 00:11:46,099
Yeah.

250
00:11:46,109 --> 00:11:49,429
And then the other thing too is like by going through this and either rewriting it or

251
00:11:49,440 --> 00:11:51,260
going through it with a fine tooth comb and

252
00:11:51,270 --> 00:11:53,460
actually putting some real resources into fixing it.

253
00:11:53,599 --> 00:11:55,419
You don't have to keep doing this firefighting.

254
00:11:55,869 --> 00:11:58,830
You know, it's the difference between putting out a little fire

255
00:11:58,989 --> 00:12:01,989
and actually going, ok, maybe we should install the sprinkler system

256
00:12:02,289 --> 00:12:03,479
and some fire exits.

257
00:12:03,830 --> 00:12:09,539
Right. Right. Exactly. And I think in security we spend so much time

258
00:12:09,859 --> 00:12:13,750
with our hair on fire just in, you know, constant panic mode

259
00:12:14,099 --> 00:12:18,400
that we never stop long enough to say if I would spend a day doing this,

260
00:12:18,409 --> 00:12:21,479
I would save myself, you know, two months of every year,

261
00:12:21,650 --> 00:12:22,679
which is huge.

262
00:12:22,859 --> 00:12:24,119
Well, perfect example of this,

263
00:12:24,659 --> 00:12:28,080
uh hands up people, how many of you want to deploy and

264
00:12:28,219 --> 00:12:32,460
enforce two factor authentication across your enterprise but haven't yet done.

265
00:12:32,469 --> 00:12:32,760
So,

266
00:12:33,070 --> 00:12:34,780
I mean, there's a lot of people that,

267
00:12:35,739 --> 00:12:36,239
right,

268
00:12:36,369 --> 00:12:37,340
basically everybody.

269
00:12:38,130 --> 00:12:41,229
Right. No, no, there's some who have already done it,

270
00:12:42,719 --> 00:12:45,989
you know, and, and that's a perfect example of all of a sudden all,

271
00:12:46,000 --> 00:12:49,979
just the whole classes of like password sniffing, password phishing,

272
00:12:49,989 --> 00:12:51,679
all that stuff just goes away,

273
00:12:51,940 --> 00:12:52,309
you know,

274
00:12:53,530 --> 00:12:56,359
with good two factor authentication, like UB keys or something. And

275
00:12:56,489 --> 00:12:58,070
how many? Yeah, so,

276
00:12:58,429 --> 00:13:02,880
you know, I get it. It's tough. We have finite resources and also to put it bluntly.

277
00:13:04,080 --> 00:13:07,260
Yeah, it's annoying if my email client keeps crashing, but

278
00:13:07,780 --> 00:13:10,150
I can just delete that email and then not look at it

279
00:13:11,239 --> 00:13:12,140
probably.

280
00:13:12,169 --> 00:13:15,979
I mean, there's, there's gonna be ways you can get around things like that. I think

281
00:13:16,609 --> 00:13:20,140
it is. And that's what makes it not that bad.

282
00:13:20,150 --> 00:13:23,340
And I feel like the other half of this

283
00:13:23,349 --> 00:13:26,880
is any organization whinging about this and saying,

284
00:13:26,890 --> 00:13:30,179
oh, I've got to work the weekend now or you've ruined my week or whatever.

285
00:13:30,890 --> 00:13:34,820
You probably have terrible security controls in place

286
00:13:35,020 --> 00:13:39,479
is my suspicion and I could be wrong and I'm welcome to

287
00:13:39,489 --> 00:13:42,640
accept whatever hate mail or tweets or whatever you want to say.

288
00:13:42,719 --> 00:13:42,729
I

289
00:13:42,849 --> 00:13:42,880
have

290
00:13:43,020 --> 00:13:44,020
a perfect example for you

291
00:13:44,549 --> 00:13:46,739
remember specter and meltdown

292
00:13:47,270 --> 00:13:48,059
and

293
00:13:48,479 --> 00:13:49,299
how

294
00:13:49,440 --> 00:13:51,700
some of the big players struggled with it. But got through it.

295
00:13:51,900 --> 00:13:53,109
And then remember, for example,

296
00:13:53,119 --> 00:13:54,549
a great example is uh some of the

297
00:13:54,559 --> 00:13:56,770
docker vulnerabilities where some providers were like,

298
00:13:56,780 --> 00:13:57,969
oh, sweet Jiminy.

299
00:13:57,979 --> 00:14:00,739
We got to like, reboot everything. We, we're up the creek, it's going to be terrible.

300
00:14:01,030 --> 00:14:03,169
And then other providers like Google

301
00:14:03,440 --> 00:14:07,619
were like, yeah, cool. We run each docker container in its own VM. So like

302
00:14:08,390 --> 00:14:08,820
neat.

303
00:14:09,190 --> 00:14:09,770
Like,

304
00:14:09,890 --> 00:14:13,200
yeah, you can break out of that darker container into the V, I'm sure you do that.

305
00:14:13,469 --> 00:14:14,380
-- Right. Right.
-- Like

306
00:14:14,559 --> 00:14:18,119
we, we just don't care like we architected our system. So

307
00:14:18,500 --> 00:14:20,700
the vast majority of Docker flaws

308
00:14:21,119 --> 00:14:24,770
-- just don't matter.
-- And that's the right way to do it,

309
00:14:25,090 --> 00:14:25,530
right

310
00:14:25,760 --> 00:14:29,150
is figure out where your threats lie and then

311
00:14:29,929 --> 00:14:31,000
eliminate them if possible.

312
00:14:32,840 --> 00:14:39,270
And so i it's clearly possible to architect things well, to build things better to,

313
00:14:39,479 --> 00:14:39,679
you know,

314
00:14:39,690 --> 00:14:42,229
there are software designs and techniques you

315
00:14:42,239 --> 00:14:45,109
can do to make things more survivable.

316
00:14:45,119 --> 00:14:48,700
I mean, and Microsoft is a great example of this things like a SLR and you know,

317
00:14:48,710 --> 00:14:49,390
all the,

318
00:14:49,510 --> 00:14:53,429
all the memory protection shenanigans they've been putting into windows so that,

319
00:14:53,659 --> 00:14:56,590
you know, exploit code used to be really easy to write. And

320
00:14:57,010 --> 00:14:59,159
I would say now it's not easy.

321
00:14:59,169 --> 00:15:01,830
It's a little bit tricky, you know, you got to jump through some hoops.

322
00:15:02,429 --> 00:15:04,969
It's very hard. I'm not gonna pretend it isn't,

323
00:15:05,219 --> 00:15:05,770
it's,

324
00:15:06,479 --> 00:15:07,700
it's not simple.

325
00:15:08,020 --> 00:15:12,679
But anyway, we are, we're off on a tangent disclosure, right?

326
00:15:12,690 --> 00:15:16,059
-- We're talking about disclosure.
-- I think it boils down to people

327
00:15:16,280 --> 00:15:18,260
are arguing against reality

328
00:15:19,190 --> 00:15:21,500
and reality doesn't really care about your feelings.

329
00:15:21,630 --> 00:15:22,659
No, it does not.

330
00:15:22,669 --> 00:15:27,880
And reality always wins, which is unfortunate but it, it does, it might lose

331
00:15:28,119 --> 00:15:29,559
in the short term occasionally.

332
00:15:29,570 --> 00:15:31,690
Which again, I, I think as I mentioned,

333
00:15:31,700 --> 00:15:33,530
while we were discussing this prior to the show,

334
00:15:33,539 --> 00:15:36,570
I feel like we're in a local minimum at the moment in the world.

335
00:15:36,580 --> 00:15:42,570
-- But long term reality will win it, it cannot lose because it is reality,
-- you know,

336
00:15:42,580 --> 00:15:44,729
well, to pivot, I mean,

337
00:15:44,940 --> 00:15:47,080
this is a perfect example of this is this whole

338
00:15:47,599 --> 00:15:52,250
French judicial analysis with A I and machine learning thing.

339
00:15:52,450 --> 00:15:54,669
Yeah, no one knows what that is, you need to explain it.

340
00:15:55,130 --> 00:15:57,969
So in France, um they don't have common law,

341
00:15:57,979 --> 00:15:59,630
they have the sort of the Napoleonic code.

342
00:15:59,640 --> 00:16:01,049
So the idea is that judges

343
00:16:01,390 --> 00:16:01,979
don't

344
00:16:02,140 --> 00:16:06,210
interpret the law, they apply the law, right? So in common law, like in,

345
00:16:06,780 --> 00:16:10,390
you know, the UK uh Canada, outside of Quebec

346
00:16:10,590 --> 00:16:13,390
and the US, right? Judges interpret the law and

347
00:16:13,520 --> 00:16:15,390
their interpretation obviously has to be

348
00:16:16,020 --> 00:16:18,400
kind of in line with how everybody else is

349
00:16:18,409 --> 00:16:20,479
interpreting the law and what the law means.

350
00:16:20,489 --> 00:16:21,940
And if it, if it's out of line, then you know,

351
00:16:21,950 --> 00:16:23,570
you appeal and you go through that whole process.

352
00:16:23,760 --> 00:16:26,400
But in France and, well, also Quebec here in Canada,

353
00:16:26,559 --> 00:16:30,200
it's essentially the judges apply the law. They are not meant to interpret the law.

354
00:16:30,780 --> 00:16:32,619
And so somebody,

355
00:16:33,059 --> 00:16:35,869
well, I'm not going to say came up with a bright idea, but basically somebody is like,

356
00:16:35,880 --> 00:16:36,530
hey, you know,

357
00:16:36,659 --> 00:16:39,530
maybe we should analyze these judges decisions

358
00:16:39,710 --> 00:16:41,789
and then we can say, you know, for law acts,

359
00:16:42,200 --> 00:16:46,820
this judge is like 85% guilty. This judge is like,

360
00:16:47,179 --> 00:16:51,979
you know, typically like 50% guilty, this judge finds people like 100% guilty.

361
00:16:51,989 --> 00:16:55,919
Right? And you can look at how they're statistically applying these laws and,

362
00:16:56,390 --> 00:16:58,190
and you know, whether or not

363
00:16:58,559 --> 00:17:01,460
they're sort of within the what should be a statistical norm, right?

364
00:17:01,469 --> 00:17:02,770
Because ideally a judge

365
00:17:03,289 --> 00:17:04,260
in a perfect world,

366
00:17:04,270 --> 00:17:05,839
you would give a court case to two different

367
00:17:05,848 --> 00:17:08,439
judges and you'd come up with basically the same answer

368
00:17:08,979 --> 00:17:09,589
in theory,

369
00:17:10,380 --> 00:17:10,880
you know,

370
00:17:11,010 --> 00:17:12,660
that would be ideal. Um

371
00:17:12,880 --> 00:17:15,930
And obviously that's not always the case because people, right, you know,

372
00:17:15,939 --> 00:17:18,339
whether or not you get the judge before or after lunch

373
00:17:18,348 --> 00:17:22,598
and they eat something spicy probably has a significant statistical impact.

374
00:17:23,150 --> 00:17:23,880
So

375
00:17:24,010 --> 00:17:25,040
France

376
00:17:25,189 --> 00:17:26,300
passed a law

377
00:17:27,900 --> 00:17:28,719
banning,

378
00:17:29,780 --> 00:17:33,000
applying analytics to judges decisions with,

379
00:17:33,010 --> 00:17:34,839
with a penalty of up to five years in prison.

380
00:17:34,989 --> 00:17:37,119
I'm sure this will totally work.

381
00:17:37,589 --> 00:17:37,709
Right.

382
00:17:37,890 --> 00:17:40,280
Well, I mean, it'll, well, here's my question.

383
00:17:40,290 --> 00:17:41,739
I was talking with somebody else and they were like,

384
00:17:41,920 --> 00:17:43,880
what happens when you have a machine learning or

385
00:17:43,890 --> 00:17:46,420
an A I essentially that sort of idea of,

386
00:17:46,430 --> 00:17:49,010
you know, fruits from the poison tree,

387
00:17:49,020 --> 00:17:51,939
you come up with some idea or statement based on

388
00:17:51,949 --> 00:17:54,630
data and analysis you're not supposed to have or do

389
00:17:54,750 --> 00:17:57,260
and you come up with a statement, say, like, you know,

390
00:17:57,270 --> 00:18:00,780
left handed people tend to buy their laundry detergents on Thursdays

391
00:18:01,760 --> 00:18:03,530
and you get rid of the original data and

392
00:18:03,540 --> 00:18:05,939
the original analysis that you used to get to that

393
00:18:06,300 --> 00:18:07,859
uh statement or that decision.

394
00:18:08,619 --> 00:18:11,199
Then what happens? Right. So somebody makes this statement,

395
00:18:12,170 --> 00:18:15,630
somebody makes a statement, you know, this judge and this judge are applying the law

396
00:18:15,989 --> 00:18:19,189
sort of outside of the statistical norm of all the other judges.

397
00:18:20,020 --> 00:18:21,579
Well, you know, how do you,

398
00:18:21,869 --> 00:18:24,920
do you, I guess you'd have to investigate and basically say, you know,

399
00:18:24,930 --> 00:18:27,520
prove to me how you figured this out.

400
00:18:27,530 --> 00:18:30,530
And if you figured it out using a computer, we're gonna put you in jail for five years.

401
00:18:30,709 --> 00:18:33,219
I understand what you're saying. So you're suggesting

402
00:18:33,550 --> 00:18:37,189
someone could acquire the end result of such analysis.

403
00:18:38,050 --> 00:18:42,560
Whereas does now does possessing, I wonder if possessing the knowledge

404
00:18:42,900 --> 00:18:44,280
is illegal.

405
00:18:44,609 --> 00:18:47,920
This is a lot like an NP complete problem in that coming up with

406
00:18:47,930 --> 00:18:51,520
the conclusion is the hard part but validating the conclusion once you have it,

407
00:18:51,530 --> 00:18:52,660
isn't that hard?

408
00:18:52,910 --> 00:18:58,010
That is true because you could just look at the court cases.

409
00:18:58,229 --> 00:18:59,140
For example,

410
00:18:59,680 --> 00:19:02,949
let's say you had a magical a IML system that identified the 1%

411
00:19:02,959 --> 00:19:06,359
of judges that were statistically outside the norm of all the other judges.

412
00:19:06,939 --> 00:19:07,719
OK.

413
00:19:07,849 --> 00:19:09,079
Printed out that list.

414
00:19:09,290 --> 00:19:10,000
OK.

415
00:19:10,300 --> 00:19:11,400
Now, you know, where to look,

416
00:19:14,040 --> 00:19:14,369
you know,

417
00:19:14,910 --> 00:19:15,630
and that's the thing.

418
00:19:15,640 --> 00:19:19,849
So now, you know, and, and the classic thing, you know, the US, um, talks,

419
00:19:19,859 --> 00:19:22,089
I believe the phrase they use is parallel discovery where,

420
00:19:22,099 --> 00:19:24,410
so they have the one stream of discovery that's, you know,

421
00:19:24,420 --> 00:19:27,239
quote unquote illegal or not legal or not allowed.

422
00:19:27,589 --> 00:19:29,839
And then they figure something out like, ok, Josh,

423
00:19:30,189 --> 00:19:34,229
you know, he's been buying detergent on Thursdays. Ok. So we figured that out.

424
00:19:34,239 --> 00:19:35,489
Now how do we sort of

425
00:19:35,609 --> 00:19:37,469
construct a chain of evidence to come

426
00:19:37,479 --> 00:19:39,050
to that conclusion without getting in trouble?

427
00:19:39,060 --> 00:19:40,729
And we could be like, hey, I got a bright idea.

428
00:19:40,739 --> 00:19:43,280
We'll just have like this FBI guy happened to be filming it

429
00:19:43,420 --> 00:19:45,829
wherever you buy detergent on a Thursday and be like, oh look,

430
00:19:45,890 --> 00:19:47,780
there's Josh buying detergent on a Thursday.

431
00:19:48,369 --> 00:19:48,400
I

432
00:19:48,540 --> 00:19:50,550
mean, I get it on the one hand, France is like,

433
00:19:50,560 --> 00:19:55,130
we want to kind of protect judges identities and they sort of,

434
00:19:55,250 --> 00:19:55,459
you know,

435
00:19:55,469 --> 00:19:58,040
we don't want them being tarred and feathered with because the other thing

436
00:19:58,050 --> 00:19:59,380
too is this also assumes that people

437
00:19:59,390 --> 00:20:01,569
are doing correct and good statistical analysis

438
00:20:01,719 --> 00:20:04,939
and statistical analysis, especially with machine learning and A I

439
00:20:05,369 --> 00:20:09,829
-- it's hard and by heart, I mean, it's damn near impossible. It's
-- hugely problematic.

440
00:20:09,839 --> 00:20:10,060
I mean,

441
00:20:10,069 --> 00:20:12,699
my favorite example I always use is that if a

442
00:20:12,709 --> 00:20:16,390
bunch of white dudes build like facial recognition systems,

443
00:20:16,520 --> 00:20:20,270
surprise, surprise, it sucks for anyone who isn't a white dude.

444
00:20:20,380 --> 00:20:20,790
I mean,

445
00:20:21,119 --> 00:20:21,739
Yeah,

446
00:20:22,160 --> 00:20:22,780
that's

447
00:20:22,880 --> 00:20:25,609
ok. And I imagine in all seriousness,

448
00:20:25,949 --> 00:20:28,270
judges are probably going to be

449
00:20:28,800 --> 00:20:30,170
in, in a similar,

450
00:20:30,500 --> 00:20:32,640
I guess bucket because let's face it.

451
00:20:33,250 --> 00:20:35,630
Judges are mostly old white dudes,

452
00:20:35,869 --> 00:20:37,689
I mean. Right. They're going to think a certain way.

453
00:20:37,699 --> 00:20:40,180
They're going to act a certain way and a lot of them don't even know it.

454
00:20:40,189 --> 00:20:43,829
You're going to run a statistical analysis and be like, oh, look, they act,

455
00:20:43,839 --> 00:20:46,359
think and talk like old white dudes, right?

456
00:20:46,369 --> 00:20:49,199
Be, be ashamed if you have some diversity here that,

457
00:20:49,209 --> 00:20:51,040
that was cleaning things up for us a little bit.

458
00:20:51,050 --> 00:20:53,959
And, and I my my suspicion as well,

459
00:20:53,969 --> 00:20:57,479
most of the people who probably created these laws are also old white dudes

460
00:20:57,819 --> 00:21:00,079
and unfortunately I have a a

461
00:21:00,339 --> 00:21:01,469
in my soul.

462
00:21:01,479 --> 00:21:04,250
I feel like this is old white dudes protecting old white

463
00:21:04,260 --> 00:21:07,439
-- dudes for being old white dudes and it's like
-- yelling at the

464
00:21:07,589 --> 00:21:10,520
-- dog,
-- don't do that. That is terrible.

465
00:21:10,530 --> 00:21:13,140
Well, see what really drives me nuts is one of the key components.

466
00:21:13,150 --> 00:21:16,400
As I understand, a fair judicial system is transparency,

467
00:21:16,859 --> 00:21:17,160
right?

468
00:21:17,170 --> 00:21:19,589
Like we're actually going through this locally where two years ago the

469
00:21:19,599 --> 00:21:23,489
Edmonton police service decided to withhold the names of homicide victims.

470
00:21:23,500 --> 00:21:27,430
Um basically making the argument that under privacy laws and under

471
00:21:27,439 --> 00:21:30,140
sort of compassion to the family and the people around them,

472
00:21:30,250 --> 00:21:32,810
you know, putting their names out in the public

473
00:21:32,969 --> 00:21:35,829
unless it serves an investigative purpose, we're not going to do it.

474
00:21:36,229 --> 00:21:36,569
And

475
00:21:37,160 --> 00:21:39,359
I get that argument. I personally disagree with it.

476
00:21:39,369 --> 00:21:41,310
And now two years later they're,

477
00:21:41,319 --> 00:21:45,119
they're backs steping on it and going back to more of a transparent.

478
00:21:45,250 --> 00:21:46,949
Yeah, we, we're going to name

479
00:21:47,109 --> 00:21:48,410
the victims of homicide

480
00:21:48,959 --> 00:21:49,670
and,

481
00:21:49,910 --> 00:21:51,069
you know, any

482
00:21:51,550 --> 00:21:56,670
legal system has to have a relative degree, if not a high degree of transparency

483
00:21:56,989 --> 00:22:00,770
because otherwise all sorts of horrible shenanigans can happen.

484
00:22:00,869 --> 00:22:02,250
You know, like in the US, there's those,

485
00:22:02,339 --> 00:22:04,489
those court cases where they found judges were

486
00:22:04,500 --> 00:22:06,569
selling Children to the to a prison system.

487
00:22:06,589 --> 00:22:08,589
-- Seriously
-- that was super messed up.

488
00:22:08,599 --> 00:22:09,400
This is,

489
00:22:09,660 --> 00:22:11,729
you know, this to me is why we need

490
00:22:11,869 --> 00:22:12,489
to

491
00:22:12,869 --> 00:22:17,170
do these kind of analysis because there are some bad people out there.

492
00:22:17,489 --> 00:22:20,270
I mean, government in general, I think needs

493
00:22:20,719 --> 00:22:22,760
this sort of transparency and

494
00:22:23,219 --> 00:22:25,050
-- I would say
-- yes and

495
00:22:25,209 --> 00:22:27,180
that's exactly where I was going with. It is

496
00:22:27,400 --> 00:22:32,260
so I, I have some stories I can tell about this around security.

497
00:22:32,619 --> 00:22:33,640
And so I,

498
00:22:33,650 --> 00:22:37,750
I work at Elastic and we're a very open organization and we have

499
00:22:37,760 --> 00:22:42,099
all this like culture stuff on the website and it's a nice place.

500
00:22:42,109 --> 00:22:46,420
I'm not going to say it isn't and security people by their nature are very secretive

501
00:22:46,760 --> 00:22:51,530
and we hired a new ciso little over a year ago who basically

502
00:22:51,540 --> 00:22:55,300
came in and is striving for radical transparency by the security groups.

503
00:22:55,910 --> 00:22:56,579
And at

504
00:22:56,790 --> 00:22:58,650
first everyone's freaking out, right?

505
00:22:58,829 --> 00:23:01,699
Like, oh we can't tell people what we're doing. It's like yes, you can.

506
00:23:01,709 --> 00:23:04,739
And you're gonna like, you know, it's not, it's not a choice.

507
00:23:04,920 --> 00:23:06,109
I remember there were a lot of

508
00:23:06,310 --> 00:23:08,790
customers that were quite shocked when I was really

509
00:23:09,000 --> 00:23:11,069
honest with them. And I'm like, yes, I, I appreciate that.

510
00:23:11,079 --> 00:23:14,069
You've reported this crosside scripting vulnerability and we're

511
00:23:14,079 --> 00:23:15,670
not going to fix it anytime soon.

512
00:23:15,770 --> 00:23:18,859
-- But, but here, here's a workaround,
-- right? And that's fair.

513
00:23:18,869 --> 00:23:21,449
But the important thing that happens is when

514
00:23:21,459 --> 00:23:24,030
you start having transparency into what you're doing,

515
00:23:24,560 --> 00:23:27,069
people ask questions and they'll say, why are you doing that?

516
00:23:27,410 --> 00:23:29,910
And then sometimes just someone asking that question of why,

517
00:23:29,920 --> 00:23:30,949
why did you do it that way?

518
00:23:30,959 --> 00:23:32,829
Why does this matter what is going on?

519
00:23:33,209 --> 00:23:34,260
And if you, you know,

520
00:23:34,270 --> 00:23:35,359
it's one of those situations where you

521
00:23:35,369 --> 00:23:37,020
start explaining something and halfway through,

522
00:23:37,030 --> 00:23:37,520
you're like,

523
00:23:37,650 --> 00:23:40,550
why am I doing this? Like this is insane

524
00:23:40,930 --> 00:23:44,280
and that's, and that's a big part of it is a lot of these large organizations,

525
00:23:44,369 --> 00:23:46,790
they don't mean or want to be bad or

526
00:23:46,800 --> 00:23:49,709
evil because like every large corporation that I hate,

527
00:23:50,280 --> 00:23:52,989
most of the people I've met that work for it are nice people.

528
00:23:53,000 --> 00:23:56,260
Of course, you know, like a great example being my phone company locally,

529
00:23:56,270 --> 00:23:57,750
I hate my phone company locally.

530
00:23:58,199 --> 00:23:58,310
I

531
00:23:58,560 --> 00:24:00,680
mean, that's just how Canada works, right? Everybody uses the

532
00:24:00,819 --> 00:24:00,859
phone.

533
00:24:01,949 --> 00:24:05,489
Everybody I know that works for this company is actually a decent human being.

534
00:24:05,959 --> 00:24:08,069
And I think that's one challenge is,

535
00:24:08,079 --> 00:24:11,310
is there's a lot of organizations that impact people's lives

536
00:24:11,670 --> 00:24:15,790
and they will start to have sort of those statistical biases or problems

537
00:24:16,020 --> 00:24:16,439
that

538
00:24:16,630 --> 00:24:18,010
basically won't show up.

539
00:24:18,020 --> 00:24:19,609
I mean, you'll kind of have a gut feeling like, oh,

540
00:24:19,619 --> 00:24:21,739
maybe we don't treat group of X people very well.

541
00:24:21,750 --> 00:24:23,439
But, like, I can't really, uh, you know,

542
00:24:23,449 --> 00:24:25,750
I don't have evidence one way or the other until

543
00:24:25,760 --> 00:24:28,310
you do something like a big statistical analysis and go,

544
00:24:28,319 --> 00:24:31,170
oh, yeah, like when it comes to group X, we're like 5%

545
00:24:31,680 --> 00:24:36,160
meaner or we charge them more money or we give them less support or

546
00:24:36,329 --> 00:24:39,130
we hang up on them more or I don't know, whatever. Right.

547
00:24:39,699 --> 00:24:41,060
And I think that's a big part of this

548
00:24:41,069 --> 00:24:43,380
is we need this transparency and we need this analysis

549
00:24:43,729 --> 00:24:45,479
so that we can do better and be better, like you said.

550
00:24:45,489 --> 00:24:46,920
So we can ask questions like, well, why,

551
00:24:46,930 --> 00:24:50,819
why do we paint the computer read before we ship to customers?

552
00:24:51,250 --> 00:24:56,640
Right. And, and we all have, it doesn't matter who you are, we all have certain

553
00:24:56,920 --> 00:25:00,920
internal biases that a lot of them, we don't even know we have.

554
00:25:01,540 --> 00:25:02,050
And

555
00:25:02,239 --> 00:25:04,329
when you have the ability to look at the data,

556
00:25:04,339 --> 00:25:07,489
it's like I forget there's some mathematician as you'll quote, if, you know,

557
00:25:07,500 --> 00:25:09,930
if you can't measure something, you can't understand it.

558
00:25:10,099 --> 00:25:10,650
And

559
00:25:11,079 --> 00:25:12,439
in a lot of ways I agree with that.

560
00:25:12,449 --> 00:25:16,050
And I feel like that's a lot of security where we just, we don't measure,

561
00:25:16,170 --> 00:25:17,489
we make assumptions

562
00:25:17,819 --> 00:25:21,050
when something fails. It's a secret. So we don't talk about it.

563
00:25:21,060 --> 00:25:24,170
So, whatever, it doesn't matter, we never, we never improve and

564
00:25:24,640 --> 00:25:25,140
that

565
00:25:25,599 --> 00:25:26,099
it creates

566
00:25:26,199 --> 00:25:27,489
-- a, it's worse
-- than we never improve.

567
00:25:27,500 --> 00:25:31,260
-- We don't even learn, we don't even know we have a problem in many cases.
-- That's true.

568
00:25:31,270 --> 00:25:31,420
Yeah,

569
00:25:31,540 --> 00:25:34,839
-- that's completely true.
-- You know, that's something I've run into a lot where

570
00:25:35,119 --> 00:25:38,500
as an outsider you look at an organization or something you're like, oh my God.

571
00:25:38,510 --> 00:25:39,650
What, like what,

572
00:25:40,709 --> 00:25:42,459
you know, and they're like, what do you mean? What?

573
00:25:42,910 --> 00:25:46,390
-- Right.
-- Of course, we, of course, everybody keeps raccoons in their bathroom.

574
00:25:47,199 --> 00:25:49,900
It's perfectly normal. Nothing to see here.

575
00:25:50,119 --> 00:25:52,180
And, and more to the point though, you can't,

576
00:25:52,510 --> 00:25:54,589
if the data is available.

577
00:25:54,670 --> 00:25:58,310
Yeah, you can legislate that people aren't allowed to analyze it,

578
00:25:58,319 --> 00:26:02,010
but you can't actually stop that from happening

579
00:26:02,199 --> 00:26:04,310
-- intentionally or unintentionally.
-- Right?

580
00:26:04,319 --> 00:26:06,680
Because there's people who don't live in France,

581
00:26:06,880 --> 00:26:09,969
Google is going to crawl through the data and their machine learning will

582
00:26:10,219 --> 00:26:13,849
stack and order the search results such that when you do certain searches,

583
00:26:14,270 --> 00:26:17,170
-- you can extract information relatively easily
-- from sure.

584
00:26:17,180 --> 00:26:19,390
Or, or there's people who don't live in France,

585
00:26:19,400 --> 00:26:21,439
there's people who do live in France who don't care.

586
00:26:21,670 --> 00:26:22,170
I mean,

587
00:26:22,290 --> 00:26:24,670
this is, I, I assume a lot of this is public data.

588
00:26:24,680 --> 00:26:27,579
You could probably download the data, crunch the numbers.

589
00:26:27,589 --> 00:26:28,670
Well, that's, that's the challenge.

590
00:26:28,680 --> 00:26:31,689
It is public data and that's why because they were saying, you know,

591
00:26:31,699 --> 00:26:32,939
we don't want to take, for example,

592
00:26:32,949 --> 00:26:35,489
the judges names off of the decisions to protect them.

593
00:26:36,280 --> 00:26:36,400
Right.

594
00:26:36,640 --> 00:26:36,800
Right.

595
00:26:37,589 --> 00:26:37,959
Yeah. And

596
00:26:38,239 --> 00:26:39,689
you definitely don't want that.

597
00:26:39,699 --> 00:26:42,890
I mean, funny enough you just made me think of something from,

598
00:26:42,900 --> 00:26:47,349
from the days of your is like, this is the old information wants to be free at it.

599
00:26:47,579 --> 00:26:48,089
Right.

600
00:26:48,290 --> 00:26:50,890
Where the point of that is obviously as, as young,

601
00:26:51,209 --> 00:26:52,859
you know, wide eyed hacker types.

602
00:26:52,869 --> 00:26:55,630
We were like, oh, we're going to make the world a better place and, well, we didn't,

603
00:26:55,640 --> 00:26:55,979
but

604
00:26:56,119 --> 00:26:59,869
the point is once information is out, you can't take it back.

605
00:26:59,880 --> 00:27:02,959
If, if someone publishes this report, whether they're in France or not,

606
00:27:02,969 --> 00:27:04,859
it doesn't matter because it's out.

607
00:27:05,050 --> 00:27:06,510
You lose what now?

608
00:27:06,760 --> 00:27:07,109
Right.

609
00:27:07,410 --> 00:27:08,410
I imagine.

610
00:27:08,839 --> 00:27:10,420
And I imagine there's a lot

611
00:27:10,760 --> 00:27:13,500
of this kind of thing going on in the world because, I mean, let's face it,

612
00:27:13,510 --> 00:27:15,420
stuff like this is actually really important.

613
00:27:15,630 --> 00:27:15,719
Uh,

614
00:27:15,729 --> 00:27:20,020
a fair and honest judicial system is like

615
00:27:20,030 --> 00:27:22,900
one of the core things of western democracy.

616
00:27:23,670 --> 00:27:24,339
Right.

617
00:27:25,469 --> 00:27:25,989
Right.

618
00:27:26,890 --> 00:27:28,550
-- I,
-- I know, I guess with that,

619
00:27:28,890 --> 00:27:30,829
uh, I guess my thought is, you know,

620
00:27:30,839 --> 00:27:33,520
why are we so afraid of being better and doing better?

621
00:27:33,780 --> 00:27:37,060
Uh, no, no, II, I think, ok,

622
00:27:37,250 --> 00:27:37,949
I'm gonna,

623
00:27:38,050 --> 00:27:39,310
I'm gonna turn that on its head.

624
00:27:39,839 --> 00:27:42,469
I don't think anyone's going to say I don't want to be better.

625
00:27:42,479 --> 00:27:45,719
I don't want to do better. Right. I think there are

626
00:27:46,109 --> 00:27:47,500
as humans.

627
00:27:48,069 --> 00:27:52,540
We have certain things we want to see and obviously self improvement is one of them.

628
00:27:52,550 --> 00:27:55,079
I don't know anyone who says I'm perfect the way I am.

629
00:27:55,219 --> 00:27:57,150
Well, there might be a couple of people and

630
00:27:57,560 --> 00:27:59,410
some of them might run countries, but

631
00:27:59,859 --> 00:28:03,380
generally speaking, most people agree that,

632
00:28:03,390 --> 00:28:05,719
that there's a certain amount of personal development they can take.

633
00:28:05,729 --> 00:28:07,270
But I think at the same time

634
00:28:07,589 --> 00:28:09,140
we have been,

635
00:28:09,939 --> 00:28:14,400
I, I don't know if it's innate or it's just something that has been

636
00:28:14,630 --> 00:28:18,099
kind of just pounded into our brains, but failure is bad,

637
00:28:18,219 --> 00:28:18,719
right?

638
00:28:19,530 --> 00:28:23,079
And so I think what happens is when you are a person in a

639
00:28:23,089 --> 00:28:28,000
culture that sees failure as a negative thing and not a positive thing.

640
00:28:28,280 --> 00:28:31,599
One of the ways you avoid failure is by hiding

641
00:28:32,050 --> 00:28:34,920
and if you're successful, then you say, oh, look what I did. Isn't this neat?

642
00:28:34,930 --> 00:28:35,640
But if you fail,

643
00:28:35,650 --> 00:28:37,510
you just sweep that under the rug and you go

644
00:28:37,520 --> 00:28:39,900
on to the next thing and everyone pretends it's cool.

645
00:28:40,619 --> 00:28:41,790
And I think that's,

646
00:28:41,890 --> 00:28:42,839
that's what we see a lot.

647
00:28:42,849 --> 00:28:46,199
We see that in security, we see that in government, we see that in,

648
00:28:46,209 --> 00:28:47,219
in almost everywhere.

649
00:28:47,229 --> 00:28:47,680
You look,

650
00:28:47,810 --> 00:28:49,380
well, you look at science, right?

651
00:28:49,410 --> 00:28:52,369
The reproducibility crisis where there's really not a lot of

652
00:28:52,380 --> 00:28:55,589
economic incentives for people to actually try and reproduce experiments.

653
00:28:55,599 --> 00:28:57,589
And then when they do, they're like, wow, this is just

654
00:28:57,810 --> 00:28:58,300
like,

655
00:28:58,420 --> 00:29:01,709
and it's not that the science is bad or wrong. It's that like, OK, your lab notes suck.

656
00:29:01,719 --> 00:29:03,650
And like, how did you go from step A to step Z?

657
00:29:03,719 --> 00:29:07,000
Because there's like the instructions are like, and then you swish it.

658
00:29:08,109 --> 00:29:08,349
Like,

659
00:29:08,619 --> 00:29:09,180
-- why?
-- What?

660
00:29:09,339 --> 00:29:13,489
No, I know. Exactly. Exactly. And this is the same thing with disclosure.

661
00:29:13,500 --> 00:29:15,329
We feel like keeping things secret,

662
00:29:15,380 --> 00:29:19,900
helps us avoid failure because now we have more time

663
00:29:19,910 --> 00:29:23,160
and potentially infinite time if we do it right.

664
00:29:23,229 --> 00:29:24,729
And then it's never a problem

665
00:29:24,910 --> 00:29:27,500
and also less customers wanting to talk to you.

666
00:29:27,729 --> 00:29:28,729
Issue X

667
00:29:28,849 --> 00:29:33,729
-- and the press is calling.
-- Right. Yeah. Exactly. Exactly. And it's like,

668
00:29:33,979 --> 00:29:37,150
like one of those situations where I love in that, you know, the comedies were like,

669
00:29:37,160 --> 00:29:38,329
nothing to see here, folks.

670
00:29:38,339 --> 00:29:40,969
Everything's fine and like the buildings exploding behind them. And

671
00:29:41,329 --> 00:29:43,469
that's totally what it is.

672
00:29:43,589 --> 00:29:47,219
Right. It's just it, and I think that's human nature and I think it is,

673
00:29:47,500 --> 00:29:49,410
it's difficult to get past that. Right.

674
00:29:49,420 --> 00:29:49,569
And,

675
00:29:49,579 --> 00:29:53,150
and some of its culture and I think some of it's just kind of personal growth and I,

676
00:29:53,160 --> 00:29:55,089
I don't have a good answer how to do it.

677
00:29:55,119 --> 00:29:55,689
I mean,

678
00:29:56,400 --> 00:29:56,890
it's,

679
00:29:57,579 --> 00:29:58,040
then

680
00:29:58,300 --> 00:30:02,079
that's probably a book. Someone could write a book about this one.

681
00:30:02,089 --> 00:30:03,510
There's already many,

682
00:30:03,849 --> 00:30:07,630
there probably are. And obviously they all work because it's the problem solved.

683
00:30:07,900 --> 00:30:08,280
That's,

684
00:30:09,439 --> 00:30:11,630
we could get into that some other day about the fact that, like,

685
00:30:11,640 --> 00:30:12,609
none of those books work.

686
00:30:12,619 --> 00:30:13,920
But that's another story.

687
00:30:14,469 --> 00:30:15,520
Uh. Right.

688
00:30:15,890 --> 00:30:17,380
All right, man. I think I'm calling it.

689
00:30:17,660 --> 00:30:19,729
So, thank you, Kurt. Thank you everyone for listening.

690
00:30:19,739 --> 00:30:21,959
You can go to open source security podcast.com. Hit up the show

691
00:30:22,069 --> 00:30:25,969
notes, you see Poundland Os S podcast hashtag to hit us up on social media.

692
00:30:26,020 --> 00:30:28,410
Kurt, have a fantastic rest of your day.

693
00:30:28,550 --> 00:30:29,819
-- You too, thanks
-- everybody.

694
00:30:30,099 --> 00:30:31,359
Thanks everyone. Bye bye

695
00:30:35,959 --> 00:30:36,020
the.